{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-21hjKp6kM4"
   },
   "source": [
    "**Note to self:** \n",
    "* args = parser.parse_args() MUST BE UNCOMMENTED WHEN USING THE HPC\n",
    "* args = parser.parse_args() MUST BE COMMENTED WHEN USING COLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bK-6g25CWED"
   },
   "source": [
    "# **Prep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_OFuZF3b5dA"
   },
   "source": [
    "## Parse command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573201169,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "64yR8CSBcJPh"
   },
   "outputs": [],
   "source": [
    "# Configure parser for command line arguments\n",
    "import argparse\n",
    "\n",
    "# Create parser\n",
    "parser = argparse.ArgumentParser(description=\"Parse command line arguments\")\n",
    "\n",
    "# Add arguments\n",
    "# Source: https://docs.python.org/3/library/argparse.html\n",
    "# Source: https://www.youtube.com/watch?v=FbEJN8FsJ9U&t=233s\n",
    "\n",
    "parser.add_argument('-b',\n",
    "                    '--batch_size',\n",
    "                    metavar='batch size',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=22,\n",
    "                    help='Enter batch size.')\n",
    "\n",
    "parser.add_argument('-d',\n",
    "                    '--device',\n",
    "                    metavar='device',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='cpu',\n",
    "                    help='Enter cuda or cpu.')\n",
    "\n",
    "parser.add_argument('-dp',\n",
    "                    '--data_parallel',\n",
    "                    metavar='data_parallel',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='N',\n",
    "                    help='Distributed inference Y/N')\n",
    "\n",
    "parser.add_argument('-f',\n",
    "                    '--filename',\n",
    "                    metavar='prompt filename',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='0shot_10fold',\n",
    "                    help='Enter prompt pickle filename.')\n",
    "\n",
    "parser.add_argument('-k',\n",
    "                    '--topk',\n",
    "                    metavar='topk',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=0,\n",
    "                    help='The K most likely next words.')\n",
    "\n",
    "parser.add_argument('-m',\n",
    "                    '--model',\n",
    "                    metavar='device',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='gpt-neo-125M',\n",
    "                    help='Enter pre-trained transformer model.')\n",
    "\n",
    "parser.add_argument('-n',\n",
    "                    '--number_outputs',\n",
    "                    metavar='number_outputs',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=1,\n",
    "                    help='Enter the number of independantly sampled outputs per problem.')\n",
    "\n",
    "parser.add_argument('-o',\n",
    "                    '--organisation',\n",
    "                    metavar='organisation',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='EleutherAI',\n",
    "                    help='Enter source (i.e. organisation) of pre-trained transformer model')\n",
    "\n",
    "parser.add_argument('-p',\n",
    "                    '--platform',\n",
    "                    metavar='platform',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='hpc',\n",
    "                    help='Enter hpc or colab.')\n",
    "\n",
    "parser.add_argument('-pr',\n",
    "                    '--topp',\n",
    "                    metavar='topp',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=10,\n",
    "                    help='Probability threshold')\n",
    "\n",
    "parser.add_argument('-t',\n",
    "                    '--temperature',\n",
    "                    metavar='temperature',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=1,\n",
    "                    help='Softmax temperature.')\n",
    "\n",
    "# Parses arguments through the parse_args() method\n",
    "\n",
    "# HPC\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Laptop\n",
    "\n",
    "# # No arguments\n",
    "# # Note: The list is passed so no arguments are passed here.\n",
    "# args = parser.parse_args([])\n",
    "\n",
    "# # top-k\n",
    "# args = parser.parse_args(['-f', '0shot_10fold', # model input\n",
    "#                           '-p', 'laptop', # platform\n",
    "#                           '-k', '10', '110', # top-k\n",
    "#                           '-t', '1', '11', # softmax-temp\n",
    "#                           '-pr', '100', '105', # probability\n",
    "#                          ])\n",
    "# top-p\n",
    "# args = parser.parse_args(['-f', '3shot_10fold_OOD_test', # model input\n",
    "#                           '-p', 'colab', # platform\n",
    "#                          '-k', '0', # top-k \n",
    "#                           '-t', '1', # softmax-temp\n",
    "#                           '-pr', '5', # top-p\n",
    "#                           '-d', 'cuda', # device\n",
    "#                           '-n', '10', # number of independantly sampled outputs\n",
    "#                           '-b', '1',\n",
    "#                          ])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573201170,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "tcaLfpw5ef1A"
   },
   "outputs": [],
   "source": [
    "# Assign variables\n",
    "b_size = args.batch_size\n",
    "data_parallel = args.data_parallel\n",
    "device_type = args.device\n",
    "filename = args.filename\n",
    "model_type = args.model\n",
    "organisation = args.organisation\n",
    "platform = args.platform\n",
    "temp = args.temperature\n",
    "topk = args.topk\n",
    "topp = args.topp\n",
    "topp = topp/10\n",
    "num_outputs = args.number_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vheIvdoCZoq"
   },
   "source": [
    "## Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22488,
     "status": "ok",
     "timestamp": 1669573223650,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "pmkgHPVMhRaZ",
    "outputId": "38d32ec4-8422-4355-fd61-a8f4938f3ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount google drive if platform\n",
    "if platform == 'colab':\n",
    "\n",
    "    # Load the Drive helper and mount\n",
    "    from google.colab import drive\n",
    "\n",
    "    # This will prompt for authorization.\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Cd to relevant google drive directory\n",
    "    # Note: Run a shell command using os.system\n",
    "    import os\n",
    "    os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wn1zUuS7fIHb"
   },
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9563,
     "status": "ok",
     "timestamp": 1669573233208,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "jxLfhtnJfLYq"
   },
   "outputs": [],
   "source": [
    "# Installations\n",
    "if platform == 'colab':\n",
    "\n",
    "    # Hugging Face\n",
    "    # Note: Run a shell command using os.system\n",
    "    import os\n",
    "    os.system(\"pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpHGVr6eBOw6"
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5365,
     "status": "ok",
     "timestamp": 1669573238565,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "9Kvu-hvlCU5y"
   },
   "outputs": [],
   "source": [
    "# Import relevant dependencies\n",
    "import bz2\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForCausalLM, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uQsJmXZBTBa"
   },
   "source": [
    "## Set torch.device\n",
    "*A torch.device is an object representing the device on which a torch.Tensor is or will be allocated.*\n",
    "\n",
    "Source: https://pytorch.org/docs/stable/tensor_attributes.html#torch.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1669573238566,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "FNJycPLaze8r",
    "outputId": "6f3b8d51-868f-4d84-c30c-97cf29ecc6ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device: cuda:0\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Set torch device\n",
    "# Source: https://pytorch.org/docs/stable/tensor_attributes.html#torch.device\n",
    "# torch_device = torch.device(device_type)\n",
    "if device_type == \"cuda\":\n",
    "    torch_device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    torch_device = torch.device(device_type)\n",
    "\n",
    "print(f\"Torch device: {torch_device}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYGWZLNr3_5L"
   },
   "source": [
    "**Note:** Data parrallelism will be implemented as per [here](https://stackoverflow.com/questions/61736317/huggingface-transformers-gpt2-generate-multiple-gpus). This means the input will be distributed across the available GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKPS_heN7OOc"
   },
   "source": [
    "## Set random number seed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1669573238566,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "Dz9d8VuEC6pZ"
   },
   "outputs": [],
   "source": [
    "# Create a function for setting/resetting the fixed seed value for pseudo-random generators\n",
    "# Source: https://odsc.medium.com/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\n",
    "# Source: https://discuss.pytorch.org/t/does-pytorch-change-its-internal-seed-during-training/46505/4\n",
    "\n",
    "def setSeedValue(seedValue):\n",
    "\n",
    "    # 1. Torch\n",
    "    # Source: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    torch.manual_seed(seedValue)\n",
    "\n",
    "    # 2. Python\n",
    "    random.seed(seedValue)\n",
    "\n",
    "    # 3. Numpy\n",
    "    # Source: https://numpy.org/doc/stable/reference/random/generated/numpy.random.seed.html\n",
    "    np.random.seed(seedValue)\n",
    "\n",
    "    # 4. HuggingFace helper function\n",
    "    # Helper function for reproducible behavior to set the seed in random , numpy , torch\n",
    "    # Source: https://huggingface.co/docs/transformers/internal/trainer_utils\n",
    "    # Used here: https://huggingface.co/Narsil/gpt2\n",
    "    set_seed(seedValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1669573238566,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "-ufn0MlrC8Ub"
   },
   "outputs": [],
   "source": [
    "# Set seed seed value for pseudo-random generators\n",
    "# Source: https://odsc.medium.com/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\n",
    "setSeedValue(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4_sErcOm_qe"
   },
   "source": [
    "## Set relevant directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1669573239062,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "NrHdEvdQIUdZ"
   },
   "outputs": [],
   "source": [
    "# Set file directories\n",
    "prompt_filename = filename + '.pickle'\n",
    "\n",
    "# Results\n",
    "time_string = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "raw_results_file_name = filename + \"-\" + \\\n",
    "    model_type + \"-\" + time_string + '_raw_results'\n",
    "results_file_name = filename + \"-\" + model_type + \"-\" + time_string + '_results'\n",
    "\n",
    "if platform == 'colab':\n",
    "\n",
    "    # Model and tokenizer directory\n",
    "    model_directory = organisation + \"/\" + model_type\n",
    "    local_model_directory = \"/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K/\" + model_type\n",
    "\n",
    "    # Prompt directory\n",
    "    prompt_directory = \"/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\"\n",
    "\n",
    "elif platform == 'hpc':\n",
    "\n",
    "    # Model and tokenizer directory (archive folder)\n",
    "    archive_folder = \"/mnt/data/users/adbz866/\"\n",
    "    local_model_directory = archive_folder + model_type\n",
    "\n",
    "    # Prompt directory (scratch folder)\n",
    "    prompt_directory = \"/mnt/scratch/users/adbz866/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUbvHCDwugGg"
   },
   "source": [
    "## Print variables and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1669573239062,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "MLc9V3y_ujDW",
    "outputId": "5c78c6ee-223f-49e7-dc80-d5954da8e65d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20221127-182038\n",
      "Prompt directory: /content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\n",
      "Prompt file name: 3shot_10fold_OOD_test.pickle\n",
      "\n",
      "\n",
      "Local model directory: /content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K/gpt-neo-125M\n",
      "Platform: colab\n",
      "Organisation: EleutherAI\n",
      "Model: gpt-neo-125M\n",
      "Device: cuda\n",
      "Top-K: 0\n",
      "Top-P: 50\n",
      "Temperature: 1\n",
      "Batch size: 1\n",
      "Data parallelisation: N\n",
      "Number of outputs per problem 10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print variables and directories\n",
    "print(\"Time:\", time_string)\n",
    "print(\"Prompt directory:\", prompt_directory)\n",
    "print(\"Prompt file name:\", prompt_filename)\n",
    "print(\"\\n\")\n",
    "print(\"Local model directory:\", local_model_directory)\n",
    "print(\"Platform:\", platform)\n",
    "print(\"Organisation:\", organisation)\n",
    "print(\"Model:\", model_type)\n",
    "print(\"Device:\", device_type)\n",
    "print(\"Top-K:\", topk)\n",
    "print(\"Top-P:\", topp)\n",
    "print(\"Temperature:\", temp)\n",
    "print(\"Batch size:\", b_size)\n",
    "print(\"Data parallelisation:\" , data_parallel)\n",
    "print(\"Number of outputs per problem\", num_outputs)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl8HwdzUULkA"
   },
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBvOFtrjLFm3"
   },
   "source": [
    "### Save/load\n",
    "\n",
    "Objects will be saved using the pickle module and the bz2 library.\n",
    "\n",
    "1. Create a pickle object using the dump() method. This serialises Python objects into a binary format\n",
    "\n",
    "2. Compress the pickle object using the bz2 library\n",
    "\n",
    "Note: Without compression, the pickle files are too large (if output scores are included, the raw output file is roughly 238 mb with 3 validation sets for only one parameter combination). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573239063,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "iUwoJSWELkJv"
   },
   "outputs": [],
   "source": [
    "# Pickle a file and then compress it\n",
    "# Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n",
    "def save_object(fname, data):\n",
    "    with bz2.open(fname, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573239064,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "LnwKfW4BPBUV"
   },
   "outputs": [],
   "source": [
    "# Load compressed pickle file\n",
    "# Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n",
    "def load_object(fname):\n",
    "    with bz2.open(fname, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMGX82klUPzA"
   },
   "source": [
    "### Delete K valued key in dict\n",
    "This can be applied to nested dictionaries of arbitrary length as it relies on a recursive function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573239064,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "SotSWwPPUcRy"
   },
   "outputs": [],
   "source": [
    "# Delete K valued key using dictionary comprehension and recursion.\n",
    "# Source of code: https://www.geeksforgeeks.org/python-remove-k-valued-key-from-nested-dictionary/\n",
    "delete_key = lambda input: { key: delete_key(value) if isinstance(value, dict) else value\n",
    "      for key, value in input.items() if key != rem_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWEziU227xSj"
   },
   "source": [
    "# Load prompts\n",
    "\n",
    "* Prompts created using NL4DV prep v3 notebook using all published NL4DV queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573239065,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "m61xG_oqYei2"
   },
   "outputs": [],
   "source": [
    "# Load queries via deseriealisation using pickle\n",
    "# Note: rb = read binary\n",
    "# Source: https://www.programiz.com/python-programming/file-operation\n",
    "# Source: https://realpython.com/python-pickle-module/\n",
    "# Source: https://ianlondon.github.io/blog/pickling-basics/\n",
    "with open(prompt_directory + \"/\" + prompt_filename, 'rb') as f:\n",
    "    test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m-A-Vy8BZci"
   },
   "source": [
    "# **Instantiate pre-trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573239065,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "deP-IGSg9cIq"
   },
   "outputs": [],
   "source": [
    "# ### Download a local copy of the model\n",
    "\n",
    "# # Instantiate the relevant model\n",
    "# local_model = AutoModelForCausalLM.from_pretrained(model_directory)\n",
    "\n",
    "# # Save a model object\n",
    "# local_model.save_pretrained(save_directory = local_model_directory)\n",
    "\n",
    "# del local_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 16114,
     "status": "ok",
     "timestamp": 1669573255171,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "I4S13FeE-byJ"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "chosen_model = AutoModelForCausalLM.from_pretrained(local_model_directory)\n",
    "chosen_model.to(torch_device)\n",
    "\n",
    "if device_type == \"cuda\" and data_parallel == \"Y\":\n",
    "    chosen_model = torch.nn.DataParallel(chosen_model, device_ids=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeCsAg-n_YMK"
   },
   "source": [
    "# **Instantiate pre-trained tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1669573255172,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "W6jDBWBR_hWP"
   },
   "outputs": [],
   "source": [
    "# ### Download a local copy of the tokenizer\n",
    "# local_tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "# local_tokenizer.save_pretrained(save_directory = local_model_directory)\n",
    "\n",
    "# del local_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1669573256850,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "E10ZCxgD_n5N",
    "outputId": "f594cea2-cb64-4910-ddf3-331bf68c67cb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the tokenizer\n",
    "chosen_tokenizer = AutoTokenizer.from_pretrained(local_model_directory)\n",
    "type(chosen_tokenizer)\n",
    "# print(chosen_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8cOIPjMBlHq"
   },
   "source": [
    "# **Tokenize model inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFm-skHRDl-P"
   },
   "source": [
    "## Configure padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1669573256851,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "92jP_6RiFgD6"
   },
   "outputs": [],
   "source": [
    "# Configure padding\n",
    "\n",
    "# 1) Tokenizer\n",
    "\n",
    "# # Print tokenizer's end of sequence token and beginning of sequence token\n",
    "# print(chosen_tokenizer.eos_token)\n",
    "# print(chosen_tokenizer.encode(chosen_tokenizer.eos_token))\n",
    "# print(chosen_tokenizer.bos_token)\n",
    "# print(chosen_tokenizer.encode(chosen_tokenizer.bos_token))\n",
    "\n",
    "# Configure tokenizer padding strategy\n",
    "chosen_tokenizer.padding_side = \"left\"\n",
    "chosen_tokenizer.pad_token = chosen_tokenizer.eos_token\n",
    "\n",
    "# 2) Model\n",
    "\n",
    "# # Print model's end of sequence token and beginning of sequence token\n",
    "# print(chosen_model.config.eos_token_id)\n",
    "# print(chosen_model.config.bos_token_id)\n",
    "\n",
    "# Configure model padding strategy\n",
    "# chosen_model.config.pad_token_id = chosen_model.config.eos_token_id\n",
    "\n",
    "if device_type == \"cuda\" and  data_parallel == \"Y\":\n",
    "    chosen_model.module.config.pad_token_id = chosen_model.module.config.eos_token_id\n",
    "\n",
    "else:\n",
    "    chosen_model.config.pad_token_id = chosen_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkHiHwI_TCiH"
   },
   "source": [
    "## Tokenization\n",
    "* Tokenize with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1669573257310,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "rGBVACmwHida",
    "outputId": "4dc7310a-7aec-4de4-f76b-ff4daacdee86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 1599])\n",
      "torch.Size([160, 1599])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize prompts (with padding) using call()\n",
    "\n",
    "# Extract inputs\n",
    "# Output: [validation set 1, validation set 2, validation set 3...] where each validation set contains a list with model inputs\n",
    "tokenized_test_set = [test_sample[-1] for test_sample in test_set]\n",
    "\n",
    "# Tokenize inputs\n",
    "# Output: [validation set 1, validation set 2, validation set 3...] where each validation set contains a tensor with tokenized inputs\n",
    "tokenized_test_set = chosen_tokenizer(\n",
    "    tokenized_test_set, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(tokenized_test_set['input_ids'].shape)\n",
    "print(tokenized_test_set['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbY0StcBBpCi"
   },
   "source": [
    "# **Inference with Top-K/P sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmVSzj8Asteg"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVcLN8vryzlR"
   },
   "source": [
    "**Sequence probabilities**\n",
    "\n",
    "* Sequence probabilities can be calculated from output scores.\n",
    "* As output scores are provided for all tokens in the model's vocab (~50,000) for each generated token, storing output scores for every test sample in all validation sets consumes a lot of memory.\n",
    "* To counter the above, sequence probabilities are calculated immediately for each validation set and then output scores are then deleted.\n",
    "* The alternative approach is to output all output scores for each token in each test sample for all validation sets. This consumes a lot of memory.\n",
    "\n",
    "Generate probabilities for individual sequences methodology:\n",
    "\n",
    "Source: https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "\n",
    "Softmax: https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1669573257311,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "TkVrI016xfNK"
   },
   "outputs": [],
   "source": [
    "# Generate sequence probability function\n",
    "# Code adapted from: https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "\n",
    "def sequence_probability(model_inputs, output_sequences, output_scores):\n",
    "\n",
    "    # Prompt length\n",
    "    # model_inputs = tokenized_prompts = {input_ids: tensor([prompt 1], [prompt 2]...), attention_mask: tensor([prompt 1], [prompt 2]...)}\n",
    "    prompt_length = model_inputs.shape[-1]\n",
    "\n",
    "    # Vocab size\n",
    "    # model_outputs = output_batch = SampleDecoderOnlyOutput([(sequences, tensor([output 1], [output 2]...), (scores, tensor([output 1], [output 2]...))]))\n",
    "    vocab_size = output_scores[0].shape[-1]\n",
    "\n",
    "    # Create sequence tensors containing only generated tokens (i.e. exclude prompt)\n",
    "    # model_outputs = output_batch = SampleDecoderOnlyOutput([(sequences, tensor([output 1], [output 2]...), (scores, tensor([output 1], [output 2]...))]))\n",
    "    model_outputs_genTokens = output_sequences[:, prompt_length:]\n",
    "\n",
    "    # Stack the logits generated at each step\n",
    "    # Note: PyTorch torch.stack() method joins (concatenates) a sequence of tensors (two or more tensors) along a new dimension.\n",
    "    model_outputs_logits = torch.stack(output_scores, dim=1)\n",
    "\n",
    "    # Calculate probabilities across the entire vocab at each step\n",
    "    model_outputs_probs = model_outputs_logits.softmax(-1)\n",
    "\n",
    "    # Collect probability of generated token\n",
    "    model_outputs_genProbs = torch.gather(\n",
    "        model_outputs_probs, 2, model_outputs_genTokens[:, :, None]).squeeze(-1)\n",
    "\n",
    "    # Calculate sequence probability\n",
    "    model_outputs_seqProbs = model_outputs_genProbs.prod(-1)\n",
    "\n",
    "    # # Print summary information\n",
    "    # print(f\"Length of padded prompts: {prompt_length}\")\n",
    "    # print(f\"Number of generated sequences: {model_outputs_genTokens.shape[0]}\")\n",
    "    # print(f\"Length of generated sequences: {model_outputs_genTokens.shape[-1]}\")\n",
    "    # print(f\"Tensor shape - Generated sequences: {model_outputs_genTokens.shape}\")\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # print(f\"Vocab size: {vocab_size}\")\n",
    "    # print(f\"Tensor shape - Vocab token logits at each step: {model_outputs_logits.shape}\")\n",
    "    # print(f\"Tensor shape - Vocab token probabilities at each step: {model_outputs_probs.shape}\")\n",
    "    # print(f\"Tensor shape - Generated token probabilities: {model_outputs_genProbs.shape}\")\n",
    "    # print(f\"Tensor shape - Generated sequence probabilities: {model_outputs_seqProbs.shape}\")\n",
    "    # print(f\"Sequence probabilities: {model_outputs_seqProbs.tolist()}\")\n",
    "\n",
    "    return model_outputs_seqProbs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnnXQY9c6PlI"
   },
   "source": [
    "The below inference function utilises generator functions to deliver batches of samples within each validation set to the model.\n",
    "\n",
    "Relevant references:\n",
    "* https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "* https://www.lachlaneagling.com/reducing-memory-consumption-python/\n",
    "* https://www.programiz.com/python-programming/generator\n",
    "* https://djangostars.com/blog/list-comprehensions-and-generator-expressions/\n",
    "* https://nolowiz.com/split-list-into-batches-using-generator-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1669573257312,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "HPd-TEiIhJxV"
   },
   "outputs": [],
   "source": [
    "# Inference function when using generator objects\n",
    "\n",
    "def create_generator(v_set, b_size):\n",
    "    for i in range(0, len(v_set), b_size):\n",
    "        yield v_set[i: i + b_size]\n",
    "\n",
    "\n",
    "def model_inference(tokenized_set):\n",
    "    output = {\n",
    "        'Inference time': None,\n",
    "        'Input sequences': None,\n",
    "        'Output sequences': None,\n",
    "        # 'Output scores': None,\n",
    "        # 'Sequence probabilities': None,\n",
    "    }\n",
    "\n",
    "    # Create two generator objects containing 1) batches of tokenized input_ids and 2) associated batches of attention masks\n",
    "    input_ids_generator = create_generator(\n",
    "        tokenized_test_set['input_ids'], b_size)\n",
    "    attn_mask_generator = create_generator(\n",
    "        tokenized_test_set['attention_mask'], b_size)\n",
    "\n",
    "    # Empty lists to collect outputs for each generator batch\n",
    "    input_batches = []\n",
    "    output_batches_sequences = []\n",
    "    # output_batches_scores = []\n",
    "\n",
    "    # Reset seed seed value for pseudo-random generators.\n",
    "    # Source: https://discuss.pytorch.org/t/does-pytorch-change-its-internal-seed-during-training/46505/4\n",
    "    setSeedValue(0)\n",
    "    start = time.time()  # Start time\n",
    "\n",
    "    # Iterate over batches in generator object\n",
    "    generator_count = 0\n",
    "\n",
    "    for input_batch, attn_batch in zip(input_ids_generator, attn_mask_generator):\n",
    "\n",
    "        generator_count += 1\n",
    "        attn_mask = attn_batch.to(torch_device)\n",
    "        batch_input = input_batch.to(torch_device)\n",
    "\n",
    "        if device_type == \"cuda\" and data_parallel == \"Y\":\n",
    "            output_batch = chosen_model.module.generate(batch_input,\n",
    "                                                        attention_mask=attn_mask,\n",
    "                                                        num_beams=1,\n",
    "                                                        do_sample=True,\n",
    "                                                        top_k=topk,\n",
    "                                                        temperature=temp,\n",
    "                                                        top_p=topp,\n",
    "                                                        early_stopping=True,\n",
    "                                                        max_new_tokens=max_length,\n",
    "                                                        eos_token_id=50256,\n",
    "                                                        # Return the prediction scores.\n",
    "                                                        output_scores=False,\n",
    "                                                        return_dict_in_generate=True,\n",
    "                                                        num_return_sequences=num_outputs,\n",
    "                                                        )\n",
    "\n",
    "            # Release GPU memory\n",
    "            # Step 1: Detach tensors, create a copy on the CPU and overwrite variables\n",
    "            # Step 2: Deleting unused objects by trigerring a manual garbage collection process and releasing all unoccupied cached memory\n",
    "            output_batch.sequences = output_batch.sequences.detach().cpu()\n",
    "            # output_batch.scores = output_batch.scores.detach().cpu()\n",
    "            attn_mask = attn_mask.detach().cpu()\n",
    "            batch_input = batch_input.detach().cpu()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elif device_type == \"cuda\" and data_parallel == \"N\":\n",
    "            output_batch = chosen_model.generate(batch_input,\n",
    "                                                 attention_mask=attn_mask,\n",
    "                                                 num_beams=1,\n",
    "                                                 do_sample=True,\n",
    "                                                 top_k=topk,\n",
    "                                                 temperature=temp,\n",
    "                                                 top_p=topp,\n",
    "                                                 early_stopping=True,\n",
    "                                                 max_new_tokens=max_length,\n",
    "                                                 eos_token_id=50256,\n",
    "                                                 # Return the prediction scores.\n",
    "                                                 output_scores=False,\n",
    "                                                 return_dict_in_generate=True,\n",
    "                                                 num_return_sequences=num_outputs,\n",
    "                                                 )\n",
    "\n",
    "            # Release GPU memory\n",
    "            # Step 1: Detach tensors, create a copy on the CPU and overwrite variables\n",
    "            # Step 2: Deleting unused objects by trigerring a manual garbage collection process and releasing all unoccupied cached memory\n",
    "            output_batch.sequences = output_batch.sequences.detach().cpu()\n",
    "            # output_batch.scores = output_batch.scores.detach().cpu()\n",
    "            attn_mask = attn_mask.detach().cpu()\n",
    "            batch_input = batch_input.detach().cpu()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        else:\n",
    "            output_batch = chosen_model.generate(batch_input,\n",
    "                                                 attention_mask=attn_mask,\n",
    "                                                 num_beams=1,\n",
    "                                                 do_sample=True,\n",
    "                                                 top_k=topk,\n",
    "                                                 temperature=temp,\n",
    "                                                 top_p=topp,\n",
    "                                                 early_stopping=True,\n",
    "                                                 max_new_tokens=max_length,\n",
    "                                                 eos_token_id=50256,\n",
    "                                                 # Return the prediction scores.\n",
    "                                                 output_scores=False,\n",
    "                                                 return_dict_in_generate=True,\n",
    "                                                 num_return_sequences=num_outputs,\n",
    "                                                 )\n",
    "\n",
    "        # Collect inputs/outputs\n",
    "        input_batches.append(batch_input)\n",
    "        output_batches_sequences.append(output_batch.sequences)\n",
    "        # output_batches_scores.append(output_batch.scores)\n",
    "        del output_batch\n",
    "\n",
    "    # Flatten inputs/outputs\n",
    "    input_batches = [in_seq for in_seq in input_batches]\n",
    "    output_batches_sequences = [\n",
    "        out_seq for out_seq in output_batches_sequences]\n",
    "    # output_batches_scores = [out_score for out_score in output_batches_scores]\n",
    "\n",
    "    # Update output dictionary\n",
    "    output['Input sequences'] = input_batches\n",
    "    output['Output sequences'] = output_batches_sequences\n",
    "    # output['Output scores'] = output_batches_scores\n",
    "    # output['Sequence probabilities'] = sequence_probability(input_batches, output_batches_sequences, output_batches_scores)\n",
    "\n",
    "    # Inference time\n",
    "    end = time.time()\n",
    "    inference_time = end - start\n",
    "    output['Inference time'] = inference_time\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BESOs_uCC1rJ"
   },
   "source": [
    "## Stopping criteria\n",
    "* The early stopping criteria will be based on *max_new_tokens* and the *EOS* token.\n",
    "\n",
    "* Max_new_tokens is estimated by determining the number of tokens in the largest specification example. 10% more tokens is then added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1669573257313,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "WZzXPjmIoG4F",
    "outputId": "8aadf5d6-b013-47a9-9e75-24d6f99f9040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\"{'$schema': 'https://vega.github.io/schema/vega-lite/v4.17.0.json', 'data': {'url': 'https://raw.githubusercontent.com/nlvcorpus/nlvcorpus.github.io/main/datasets/movies.csv'}, 'mark': {'type': 'bar', 'tooltip': None}, 'encoding': {'column': {'field': 'Content Rating', 'type': 'ordinal'}, 'x': {'field': 'Creative Type', 'scale': {}, 'type': 'nominal', 'axis': {'title': '', 'labels': False, 'ticks': False}}, 'y': {'aggregate': 'mean', 'field': 'Production Budget', 'type': 'quantitative', 'axis': {'title': 'AVG (Production Budget)', 'format': '~s'}}, 'color': {'field': 'Creative Type', 'type': 'nominal'}}}\"\n",
      "139\n",
      "\"{'$schema': 'https://vega.github.io/schema/vega-lite/v4.17.0.json', 'mark': {'type': 'point', 'tooltip': True}, 'encoding': {'x': {'field': 'Worldwide Gross', 'type': 'quantitative', 'aggregate': None, 'axis': {'format': 's'}}, 'y': {'field': 'Production Budget', 'type': 'quantitative', 'aggregate': None, 'axis': {'format': 's'}}, 'tooltip': {'field': 'Title'}}, 'transform': [{'filter': {'field': 'Release Year', 'range': ['1990/01/01', '2000/01/01']}}, {'filter': {'field': 'Genre', 'oneOf': ['Romantic Comedy']}}], 'data': {'url': 'https://raw.githubusercontent.com/nl4dv/nl4dv/master/examples/assets/data/movies-w-year.csv', 'format': {'type': 'csv'}}}\"\n",
      "Max output length: 284.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate maximum specification length\n",
    "\n",
    "# Create a single list containing all examples.\n",
    "# test_set = [(query, query_type, dataset, data_url, spec, context, prompt, input)...]\n",
    "flattened_specs = [json.dumps(spec) for query, query_type, dataset, data_url, metaData, spec, markType, context, prompt, input in test_set]\n",
    "\n",
    "# Determine max spec length after tokenization\n",
    "max_length = 0\n",
    "i = 0\n",
    "\n",
    "for spec in flattened_specs:\n",
    "    i += 1\n",
    "    length = chosen_tokenizer(json.dumps(\n",
    "        spec), return_tensors=\"pt\", padding=False).input_ids.shape[1]\n",
    "    # print(length)\n",
    "\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "        print(i)\n",
    "        print(spec)\n",
    "\n",
    "# print(max_length)\n",
    "# Max_new_tokens = max length + 10%\n",
    "max_length += max_length*0.1\n",
    "max_length = round(max_length, 0)\n",
    "print(f\"Max output length: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB-tfmB2H9pN"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1573107,
     "status": "ok",
     "timestamp": 1669574830411,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "9jr822GGY11l"
   },
   "outputs": [],
   "source": [
    "# Conduct inference\n",
    "output_test_set = model_inference(tokenized_test_set)\n",
    "# print(output_test_set.keys())\n",
    "# print(len(output_test_set['Output sequences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhalBnE0AzY6"
   },
   "source": [
    "## Save raw output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7390,
     "status": "ok",
     "timestamp": 1669574837791,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "QM1IcMrlISR8",
    "outputId": "f33d54c9-df07-4fdf-dc30-062692790e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3shot_10fold_OOD_test-gpt-neo-125M-20221127-182038_raw_results\n"
     ]
    }
   ],
   "source": [
    "# Save via seriealisation and compression using pickle using bz2\n",
    "print(raw_results_file_name)\n",
    "save_object(raw_results_file_name, output_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de2TrGS0AVYi"
   },
   "source": [
    "# **Process outputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tB2AKSAy0_-q"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1669574837793,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "JoYvggsA1Bjj"
   },
   "outputs": [],
   "source": [
    "# Split list function\n",
    "# Source: https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "def break_list(l, n):\n",
    "    \"\"\" Turn a list into a list of lists with size n\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1669574837794,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "VEnWGhXfF47n"
   },
   "outputs": [],
   "source": [
    "# Decode function\n",
    "def decode_outputs(raw_sequences):\n",
    "    \"\"\"Decode raw output sequences for a given batch\"\"\"\n",
    "    return chosen_tokenizer.decode(raw_sequences, skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCjPhZKVXndc"
   },
   "source": [
    "## Decode outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1669574838922,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "qOf_L2AmAcFA"
   },
   "outputs": [],
   "source": [
    "# Decode raw output generated by the model\n",
    "output_test_set['Decoded sequences'] = []\n",
    "\n",
    "# Decode\n",
    "for batch in output_test_set['Output sequences']:\n",
    "    output_test_set['Decoded sequences'].append(\n",
    "        [decode_outputs(output) for output in batch])\n",
    "\n",
    "# Flatten\n",
    "output_test_set['Decoded sequences'] = [\n",
    "    decode for batch in output_test_set['Decoded sequences'] for decode in batch]\n",
    "\n",
    "# Group together independantly sampled outputs\n",
    "if num_outputs > 1:\n",
    "    output_test_set['Decoded sequences'] = list(break_list(\n",
    "        output_test_set['Decoded sequences'], num_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De5DSDEIuFWO"
   },
   "source": [
    "## Reduce memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05Goj4B0cuXn"
   },
   "source": [
    "To save memory, raw output sequences, output scores and raw input sequences are removed from the output_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1669574838923,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "J7Z-R3UWcuX5"
   },
   "outputs": [],
   "source": [
    "# Delete specified keys\n",
    "for rem_key in ['Output scores', 'Input sequences', 'Output sequences']:\n",
    "    output_test_set = delete_key(output_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohA6uB0_pKVX"
   },
   "source": [
    "## Save decoded output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3936,
     "status": "ok",
     "timestamp": 1669574842851,
     "user": {
      "displayName": "Billy Pitchford",
      "userId": "01251209915329143145"
     },
     "user_tz": 0
    },
    "id": "I0WssELWpLsQ",
    "outputId": "d6bd0dd2-e7c5-4c78-a27f-b73ad025af6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3shot_10fold_OOD_test-gpt-neo-125M-20221127-182038_results\n"
     ]
    }
   ],
   "source": [
    "# Save via seriealisation and compression using pickle using bz2\n",
    "print(results_file_name)\n",
    "save_object(results_file_name, output_test_set)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZBvOFtrjLFm3",
    "pMGX82klUPzA"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
