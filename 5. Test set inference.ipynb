{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d314eb23-032a-4eeb-b6d2-4795b2b0cbed",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a733034-e959-46c5-a45d-b00eaf57a82a",
   "metadata": {},
   "source": [
    "## Select Colab or HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac0705a-94f2-403c-af87-5817d9bb5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLATFORM = 'colab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714a554-a9ae-4f23-b9d0-20fc05853c08",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7080ecde-97d8-46d3-b65e-ec8b19bc79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bz2\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01365030-cfed-45cb-826f-aa57f2bd0b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Mount Google Drive and CD using a shell command\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      9\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "if PLATFORM == 'colab':\n",
    "    \n",
    "    # Install Hugging Face library using a shell command\n",
    "    import os\n",
    "    os.system(\"pip install transformers\")\n",
    "\n",
    "    # Mount Google Drive and CD using a shell command\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ee1580f-474a-4365-99cf-1a17727071e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForCausalLM, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e4f3c-cd18-4ffe-b6b0-5ef9c9a0329b",
   "metadata": {},
   "source": [
    "## Top level functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2563f6-c7ef-4b04-96f6-e3734fdaed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_list(l, n):\n",
    "    \"\"\" Turn a list into a list of lists with size n\n",
    "    Source: https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "\n",
    "def decode_outputs(raw_sequences):\n",
    "    \"\"\"Decode raw output sequences for a given batch\"\"\"\n",
    "    return chosen_tokenizer.decode(raw_sequences, skip_special_tokens=False)\n",
    "\n",
    "\n",
    "def save_object(fname, data):\n",
    "    \"\"\"Pickle a file and compress it.\n",
    "    Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n",
    "    \"\"\"\n",
    "    with bz2.open(fname, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_object(fname):\n",
    "    \"\"\"Load compressed pickle file\n",
    "    Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n",
    "    \"\"\"\n",
    "    with bz2.open(fname, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"Delete K valued key using dictionary comprehension and recursion. \n",
    "Source of code: https://www.geeksforgeeks.org/python-remove-k-valued-key-from-nested-dictionary\n",
    "\"\"\"\n",
    "delete_key = lambda input: {key: delete_key(value) if isinstance(value, dict) else value\n",
    "      for key, value in input.items() if key != rem_key}\n",
    "\n",
    "\n",
    "def set_seed_value(seed_value):\n",
    "    \"\"\"Create a function for setting/resetting the fixed seed value for pseudo-random generators.\n",
    "    Source: https://odsc.medium.com/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\n",
    "    Source: https://discuss.pytorch.org/t/does-pytorch-change-its-internal-seed-during-training/46505/4\n",
    "    Source: https://huggingface.co/docs/transformers/internal/trainer_utils\n",
    "    Source: https://huggingface.co/Narsil/gpt2\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed_value) # 1. Torch\n",
    "    random.seed(seed_value) # 2. Python\n",
    "    np.random.seed(seed_value) # 3. Numpy\n",
    "    # 4. HuggingFace helper function to set the seed in random , numpy , torch\n",
    "    set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "457ac7eb-74d2-43f4-bb88-5377f7792d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(tokenized_inputs, BATCH_SIZE):\n",
    "    \"\"\"Generator object used to deliver batches of the tokenized inputs\n",
    "    and their associated attention masks to the model thereby reducing memory consumption. \n",
    "    \"\"\"\n",
    "    for i in range(0, len(tokenized_inputs), BATCH_SIZE):\n",
    "        yield tokenized_inputs[i: i + BATCH_SIZE]\n",
    "\n",
    "\n",
    "def model_inference(tokenized_inputs, BATCH_SIZE, TEMPERATURE, TOPK, TOPP, NUM_OUTPUTS, MAX_OUTPUT_LENGTH):\n",
    "    \"\"\"This function enables batched inference using the chosen model on either the CPU or GPU.\n",
    "    The latter includes optional parallelisation.\"\"\"\n",
    "    set_seed_value(0)\n",
    "    inference_results = {'Inference time': None,\n",
    "                         'Output sequences': None\n",
    "                        }\n",
    "    model_outputs = []\n",
    "\n",
    "    # Iterate over generator objects\n",
    "    input_ids_generator = batch_generator(tokenized_test_set['input_ids'],\n",
    "                                           BATCH_SIZE\n",
    "                                          )\n",
    "    attn_mask_generator = batch_generator(tokenized_test_set['attention_mask'],\n",
    "                                           BATCH_SIZE\n",
    "                                          )\n",
    "    start = time.time()\n",
    "    generator_count = 0\n",
    "    for input_batch, attn_batch in zip(input_ids_generator, attn_mask_generator):\n",
    "        generator_count += 1\n",
    "        input_batch = input_batch.to(TORCH_DEVICE)\n",
    "        attn_batch = attn_batch.to(TORCH_DEVICE)\n",
    "\n",
    "        if DEVICE_TYPE == \"cuda\" and DATA_PARALLEL == \"Y\":\n",
    "            output = chosen_model.module.generate(input_batch,\n",
    "                                                  attention_mask=attn_batch,\n",
    "                                                  num_beams=1,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_k=TOPK,\n",
    "                                                  temperature=TEMPERATURE,\n",
    "                                                  top_p=TOPP,\n",
    "                                                  early_stopping=True,\n",
    "                                                  max_new_tokens=MAX_OUTPUT_LENGTH,\n",
    "                                                  eos_token_id=50256,\n",
    "                                                  output_scores=False,\n",
    "                                                  return_dict_in_generate=True,\n",
    "                                                  num_return_sequences=NUM_OUTPUTS,\n",
    "                                                 )\n",
    "\n",
    "            # Release GPU memory\n",
    "            # Step 1: Detach tensors, create a copy on the CPU and overwrite variables\n",
    "            # Step 2: Deleting unused objects by trigerring a manual garbage collection process and releasing all unoccupied cached memory\n",
    "            output.sequences = output.sequences.detach().cpu()\n",
    "            attn_batch = attn_batch.detach().cpu()\n",
    "            input_batch = input_batch.detach().cpu()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        elif DEVICE_TYPE == \"cuda\" and DATA_PARALLEL == \"N\":\n",
    "            output = chosen_model.generate(input_batch,\n",
    "                                           attention_mask=attn_batch,\n",
    "                                           num_beams=1,\n",
    "                                           do_sample=True,\n",
    "                                           top_k=TOPK,\n",
    "                                           temperature=TEMPERATURE,\n",
    "                                           top_p=TOPP,\n",
    "                                           max_new_tokens=MAX_OUTPUT_LENGTH,\n",
    "                                           eos_token_id=50256,\n",
    "                                           output_scores=False,\n",
    "                                           return_dict_in_generate=True,\n",
    "                                           num_return_sequences=NUM_OUTPUTS,\n",
    "                                          )\n",
    "\n",
    "            # Release GPU memory\n",
    "            # Step 1: Detach tensors, create a copy on the CPU and overwrite variables\n",
    "            # Step 2: Deleting unused objects by trigerring a manual garbage collection process and releasing all unoccupied cached memory\n",
    "            output.sequences = output.sequences.detach().cpu()\n",
    "            attn_mask = attn_mask.detach().cpu()\n",
    "            input_ids = input_ids.detach().cpu()\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        else:\n",
    "            output = chosen_model.generate(input_batch,\n",
    "                                           attention_mask=attn_batch,\n",
    "                                           num_beams=1,\n",
    "                                           do_sample=True,\n",
    "                                           top_k=TOPK,\n",
    "                                           temperature=TEMPERATURE,\n",
    "                                           top_p=TOPP,\n",
    "                                           early_stopping=True,\n",
    "                                           max_new_tokens=MAX_OUTPUT_LENGTH,\n",
    "                                           eos_token_id=50256,\n",
    "                                           output_scores=False,\n",
    "                                           return_dict_in_generate=True,\n",
    "                                           num_return_sequences=NUM_OUTPUTS,\n",
    "                                          )\n",
    "            \n",
    "        model_outputs.append(output.sequences)\n",
    "        del output\n",
    "\n",
    "    end = time.time()\n",
    "    inference_time = end - start\n",
    "    inference_results['Inference time'] = inference_time\n",
    "    model_outputs = [output_sequence for output_sequence in model_outputs]\n",
    "    inference_results['Output sequences'] = model_outputs\n",
    "    return inference_results\n",
    "\n",
    "\n",
    "def sequence_probability(tokenized_inputs, output_sequences, output_scores):\n",
    "    \"\"\"Calculate sequence probabilities from output scores.\n",
    "    Code adapted from: 1) https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "    2) https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F17187507%2Fwhy-use-softmax-as-opposed-to-standard-normalization\n",
    "    Note:\n",
    "    model_inputs = tokenized_prompts = {input_ids: tensor([prompt 1], [prompt 2]...), attention_mask: tensor([prompt 1], [prompt 2]...)}\n",
    "    model_outputs = output_batch = SampleDecoderOnlyOutput([(sequences, tensor([output 1], [output 2]...), (scores, tensor([output 1], [output 2]...))]))\n",
    "    \"\"\"\n",
    "    prompt_length = tokenized_inputs.shape[-1]\n",
    "    vocab_size = output_scores[0].shape[-1]\n",
    "    generated_tokens = output_sequences[:, prompt_length:] # generated tokens ONLY (i.e. exclude prompt)\n",
    "\n",
    "    # Stack the logits generated at each step and calculate probabilities across the entire vocab at each step\n",
    "    # Note: PyTorch torch.stack() method joins (concatenates) a sequence of tensors (two or more tensors) along a new dimension.\n",
    "    vocab_logits = torch.stack(output_scores, dim=1)\n",
    "    vocab_probs = logits.softmax(-1)\n",
    "\n",
    "    # Collect probability of generated tokens & calculate sequence probability\n",
    "    generated_tokens_probs  = torch.gather(vocab_probs, 2,\n",
    "                                          generated_tokens[:, :, None]).squeeze(-1)\n",
    "    generated_sequence_probs = generated_tokens_probs.prod(-1)\n",
    "\n",
    "    # # Print summary information\n",
    "    # print(f\"Length of padded prompts: {prompt_length}\")\n",
    "    # print(f\"Number of generated sequences: {generated_tokens.shape[0]}\")\n",
    "    # print(f\"Length of generated sequences: {generated_tokens.shape[-1]}\")\n",
    "    # print(f\"Tensor shape - Generated sequences: {generated_tokens.shape}\")\n",
    "    # print(\"\\n\")\n",
    "    # print(f\"Vocab size: {vocab_size}\")\n",
    "    # print(f\"Tensor shape - Vocab token logits at each step: {vocab_logits.shape}\")\n",
    "    # print(f\"Tensor shape - Vocab token probabilities at each step: {vocab_logits.shape}\")\n",
    "    # print(f\"Tensor shape - Generated token probabilities: {generated_tokens_probs.shape}\")\n",
    "    # print(f\"Tensor shape - Generated sequence probabilities: {generated_sequence_probs.shape}\")\n",
    "    # print(f\"Sequence probabilities: {generated_sequence_probs.tolist()}\")\n",
    "    \n",
    "    return generated_sequence_probs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a42cd-57fc-4396-903a-83984744aec0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9151368-0369-4777-9ca0-fd2e02271efd",
   "metadata": {},
   "source": [
    "### Command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa3f3f29-89e7-40a1-9a6c-557a20d2ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parser for command line arguments\n",
    "# Source: https://docs.python.org/3/library/argparse.html\n",
    "parser = argparse.ArgumentParser(description=\"Parse command line arguments\")\n",
    "parser.add_argument('-b',\n",
    "                    '--batch_size',\n",
    "                    metavar='batch size',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=22,\n",
    "                    help='Batch size.')\n",
    "\n",
    "parser.add_argument('-dp',\n",
    "                    '--data_parallel',\n",
    "                    metavar='data_parallel',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='N',\n",
    "                    help='Distributed inference Y/N.')\n",
    "\n",
    "parser.add_argument('-d',\n",
    "                    '--device',\n",
    "                    metavar='device',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='cpu',\n",
    "                    help='Cuda or cpu.')\n",
    "\n",
    "parser.add_argument('-f',\n",
    "                    '--filename',\n",
    "                    metavar='prompt filename',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='ID_3shot_test_set',\n",
    "                    help='Input filename.')\n",
    "\n",
    "parser.add_argument('-m',\n",
    "                    '--model',\n",
    "                    metavar='device',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='gpt-neo-125M',\n",
    "                    help='Pre-trained transformer model.')\n",
    "\n",
    "parser.add_argument('-n',\n",
    "                    '--number_outputs',\n",
    "                    metavar='number_outputs',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=1,\n",
    "                    help='The number of independantly sampled outputs per problem.')\n",
    "\n",
    "parser.add_argument('-t',\n",
    "                    '--temperature',\n",
    "                    metavar='temperature',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=1,\n",
    "                    help='Softmax temperature.')\n",
    "\n",
    "parser.add_argument('-k',\n",
    "                    '--topk',\n",
    "                    metavar='topk',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=0,\n",
    "                    help='The K most likely next words.')\n",
    "\n",
    "parser.add_argument('-pr',\n",
    "                    '--topp',\n",
    "                    metavar='topp',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=10,\n",
    "                    help='Probability threshold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8023328-5a7f-49df-9067-43410fcfca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLATFORM=='colab':\n",
    "    # args = parser.parse_args([])\n",
    "    args = parser.parse_args(['--batch_size', '1',\n",
    "                              '--data_parallel', 'N',\n",
    "                              '--device', 'cpu',\n",
    "                              '--filename', 'ID_3shot_test_set',\n",
    "                              '--model', 'gpt-neo-125M',\n",
    "                              '--number_outputs', '1',\n",
    "                              '--temperature', '1',\n",
    "                              '--topk', '0',\n",
    "                              '--topp', '5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4bf450-cacb-4693-a176-cf015895df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses arguments through the parse_args() method\n",
    "if PLATFORM=='HPC':\n",
    "    args = parser.parse_args()\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "DATA_PARALLEL = args.data_parallel\n",
    "DEVICE_TYPE = args.device\n",
    "FILENAME = args.filename\n",
    "MODEL_TYPE = args.model\n",
    "NUM_OUTPUTS = args.number_outputs\n",
    "TEMPERATURE = args.temperature\n",
    "TOPK = args.topk\n",
    "TOPP = args.topp/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f51898-9382-4238-af2e-082cf23e8b47",
   "metadata": {},
   "source": [
    "### File parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c750cb28-2228-4fd3-9b01-e23190e6ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STRING = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "PROMPT_FILENAME = FILENAME + '.pickle'\n",
    "RAW_RESULTS_FILENAME = FILENAME + \"-\" + MODEL_TYPE + \"-\" + TIME_STRING + '_raw_results'\n",
    "RESULTS_FILENAME = FILENAME + \"-\" + MODEL_TYPE + \"-\" + TIME_STRING + '_results'\n",
    "\n",
    "if PLATFORM == 'colab':\n",
    "    MODEL_DIRECTORY =  'EleutherAI/' + MODEL_TYPE\n",
    "    LOCAL_MODEL_DIRECTORY = 'C:/Users/billy/OneDrive/Documents/Python Scripts/1. Portfolio/1. NL2VIS/' + MODEL_TYPE\n",
    "    PROMPT_DIRECTORY = 'C:/Users/billy/OneDrive/Documents/Python Scripts/1. Portfolio/1. NL2VIS/'\n",
    "\n",
    "elif platform == 'hpc':\n",
    "    ARCHIVE_FOLDER = \"/mnt/data/users/adbz866/\"\n",
    "    LOCAL_MODEL_DIRECTORY = archive_folder + model_type\n",
    "    PROMPT_DIRECTORY = \"/mnt/scratch/users/adbz866/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3812962-259f-4a9b-90d4-7cb26a4fc101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20230829-125135\n",
      "Prompt directory: C:/Users/billy/OneDrive/Documents/Python Scripts/1. Portfolio/1. NL2VIS/\n",
      "Prompt file name: ID_3shot_test_set.pickle\n",
      "\n",
      "\n",
      "Local model directory: C:/Users/billy/OneDrive/Documents/Python Scripts/1. Portfolio/1. NL2VIS/gpt-neo-125M\n",
      "Platform: colab\n",
      "Model: gpt-neo-125M\n",
      "Device: cpu\n",
      "Top-K: 0\n",
      "Top-P: 0.5\n",
      "Temperature: 1\n",
      "Batch size: 1\n",
      "Data parallelisation: N\n",
      "Number of outputs per problem 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Time:\", TIME_STRING)\n",
    "print(\"Prompt directory:\", PROMPT_DIRECTORY)\n",
    "print(\"Prompt file name:\", PROMPT_FILENAME)\n",
    "print(\"\\n\")\n",
    "print(\"Local model directory:\", LOCAL_MODEL_DIRECTORY)\n",
    "print(\"Platform:\", PLATFORM)\n",
    "print(\"Model:\", MODEL_TYPE)\n",
    "print(\"Device:\", DEVICE_TYPE)\n",
    "print(\"Top-K:\", TOPK)\n",
    "print(\"Top-P:\", TOPP)\n",
    "print(\"Temperature:\", TEMPERATURE)\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Data parallelisation:\" , DATA_PARALLEL)\n",
    "print(\"Number of outputs per problem\", NUM_OUTPUTS )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260286ca-da8c-46ca-a7bf-629851e979c7",
   "metadata": {},
   "source": [
    "## Torch device\n",
    "* The torch.device objects represents the device on which torch.tensors will be allocated.ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc8bbb9-42ea-43c0-a0ef-260d821a4539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device: cpu\n",
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "# Source: https://pytorch.org/docs/stable/tensor_attributes.html#torch.device\n",
    "if DEVICE_TYPE == \"cuda\":\n",
    "    TORCH_DEVICE = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    TORCH_DEVICE = torch.device(DEVICE_TYPE)\n",
    "\n",
    "print(f\"Torch device: {TORCH_DEVICE}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f682508-0cd9-447c-8cac-5d06ca94602c",
   "metadata": {},
   "source": [
    "# Instantiate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8690e-3d76-4b68-aac8-3002e3456ab6",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "* Tokenize with padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db222edf-e4fd-49be-b1f5-d622b3b8790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "# local_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "# local_tokenizer.save_pretrained(save_directory=LOCAL_MODEL_DIRECTORY)\n",
    "# del local_tokenizer\n",
    "\n",
    "chosen_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIRECTORY)\n",
    "print(type(chosen_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56dc87-8121-4aa3-8d5c-3cb0d6e7385d",
   "metadata": {},
   "source": [
    "## Pre-trained transformer\n",
    "* Data parallelism is implemented as per [here](https://stackoverflow.com/questions/61736317/huggingface-transformers-gpt2-generate-multiple-gpus). This means the input will be distributed across the available GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2c09541-21f9-4d7b-a153-2f15bf5ca3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model = AutoModelForCausalLM.from_pretrained(MODEL_DIRECTORY)\n",
    "# local_model.save_pretrained(save_directory=LOCAL_MODEL_DIRECTORY)\n",
    "# del local_model\n",
    "\n",
    "chosen_model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_DIRECTORY)\n",
    "chosen_model.to(TORCH_DEVICE)\n",
    "if DEVICE_TYPE == \"cuda\" and DATA_PARALLEL == \"Y\":\n",
    "    chosen_model = torch.nn.DataParallel(chosen_model, device_ids=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa016c47-fba5-4663-ac61-982b06ca8d58",
   "metadata": {},
   "source": [
    "## Configure padding\n",
    "* EOS = end of sequence token\n",
    "* BOS = beginning of sequence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "531a308e-a9a3-4a14-9638-97c7db5a066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "[50256]\n",
      "<|endoftext|>\n",
      "[50256]\n",
      "50256\n",
      "50256\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "chosen_tokenizer.padding_side = \"left\"\n",
    "chosen_tokenizer.pad_token = chosen_tokenizer.eos_token\n",
    "print(chosen_tokenizer.eos_token)\n",
    "print(chosen_tokenizer.encode(chosen_tokenizer.eos_token))\n",
    "print(chosen_tokenizer.bos_token)\n",
    "print(chosen_tokenizer.encode(chosen_tokenizer.bos_token))\n",
    "\n",
    "# Model\n",
    "if DEVICE_TYPE == \"cuda\" and  DATA_PARALLEL == \"Y\":\n",
    "    chosen_model.module.config.pad_token_id = chosen_model.module.config.eos_token_id\n",
    "else:\n",
    "    chosen_model.config.pad_token_id = chosen_model.config.eos_token_id\n",
    "print(chosen_model.config.eos_token_id)\n",
    "print(chosen_model.config.bos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510f63d-bfd1-44da-9975-9b609b52f162",
   "metadata": {},
   "source": [
    "# Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "959249f6-c40c-4328-878e-619b94bad244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data via deseriealisation using pickle\n",
    "with open(PROMPT_DIRECTORY + \"/\" + PROMPT_FILENAME, 'rb') as f:\n",
    "    test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18930f7-58f2-4b5d-b462-1a0d6a38437e",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e60ca3-4601-4fe5-87b1-0f783577cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([149, 1663])\n",
      "torch.Size([149, 1663])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize prompts (with padding) using call()\n",
    "tokenized_test_set = test_set['model_input'].tolist()\n",
    "tokenized_test_set = chosen_tokenizer(tokenized_test_set, \n",
    "                                      return_tensors=\"pt\", \n",
    "                                      padding=True)\n",
    "\n",
    "print(tokenized_test_set['input_ids'].shape)\n",
    "print(tokenized_test_set['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9b2e9-830b-47cd-a61f-515ecf161a1a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7153c70-c3be-4e6e-9ae6-a30d00e5900c",
   "metadata": {},
   "source": [
    "## Stopping criteria\n",
    "* The early stopping criteria is based on *max_new_tokens* and the *EOS* token.\n",
    "\r\n",
    "* Max_new_tokens is estimated by determining the number of tokens in the largest specification example. 10% more tokens is then added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94a164cb-e138-4008-aeb8-1e8890a5a11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max output length: 282.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate maximum specification length\n",
    "i = 0\n",
    "MAX_OUTPUT_LENGTH = 0\n",
    "for spec in test_set['spec'].tolist():\n",
    "    i += 1\n",
    "    length = chosen_tokenizer(json.dumps(spec),\n",
    "                              return_tensors=\"pt\",\n",
    "                              padding=False).input_ids.shape[1]\n",
    "    \n",
    "    if length > MAX_OUTPUT_LENGTH:\n",
    "        MAX_OUTPUT_LENGTH = length\n",
    "        \n",
    "MAX_OUTPUT_LENGTH += MAX_OUTPUT_LENGTH*0.1\n",
    "MAX_OUTPUT_LENGTH = round(MAX_OUTPUT_LENGTH, 0)\n",
    "print(f\"Max output length: {MAX_OUTPUT_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72307257-e970-4a78-b227-88cbc7be2430",
   "metadata": {},
   "source": [
    "## Inference\r\n",
    "\r\n",
    "Sequence probabilities are not output below. However, they can be using the *sequence_probabilities()* function defined earlier. In this instance, note the below:\r\n",
    "* Sequence probabilities are calculated from output scores.\r\n",
    "* However, for each generated token, output scores are provided across all tokens in the model's vocab (~50,000). Storing output scores for every sample therefore consumes a lot of memory.\r\n",
    "* To counter this, calculate sequence probabilities immediately for each batch before deleting output scores using the function provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc358d-a610-4a10-961e-23ca2088c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_results = model_inference(tokenized_test_set, BATCH_SIZE, TEMPERATURE, TOPK, TOPP, NUM_OUTPUTS, MAX_OUTPUT_LENGTH)\n",
    "# print(inference_results.keys())\n",
    "# print(len(inference_results['Output sequences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58bf279-48d2-4cb0-a2d0-2a915e17de00",
   "metadata": {},
   "source": [
    "# Process outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f7eb4-f324-426a-9333-2f75c56781ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode & flatten batches\n",
    "inference_results['Decoded sequences'] = []\n",
    "for batch in inference_results['Output sequences']:\n",
    "    inference_results['Decoded sequences'].append([decode_outputs(output) for output in batch])\n",
    "output_test_set['Decoded sequences'] = [decode for batch in output_test_set['Decoded sequences'] for decode in batch]\n",
    "\n",
    "# Group together independantly sampled outputs\n",
    "if num_outputs > 1:\n",
    "    inference_results['Decoded sequences'] = list(break_list(inference_results['Decoded sequences'],\n",
    "                                                             num_outputs\n",
    "                                                            )\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa2454-bc80-4209-9a51-5e3eddee4e2c",
   "metadata": {},
   "source": [
    "# Save outputs\n",
    "* To save memory, unnecessary keys are removed from the output containing decoded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7612acb-d164-4c6d-b71d-3256aa4a1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "save_object(RAW_RESULTS_FILENAME,\n",
    "            inference_results)\n",
    "\n",
    "# Decoded output\n",
    "for rem_key in ['Output scores', 'Input sequences', 'Output sequences']:\n",
    "    inference_results = delete_key(inference_results)\n",
    "save_object(RESULTS_FILENAME,\n",
    "            inference_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2vis",
   "language": "python",
   "name": "nl2vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
