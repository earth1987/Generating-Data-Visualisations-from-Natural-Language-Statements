{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d314eb23-032a-4eeb-b6d2-4795b2b0cbed",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a733034-e959-46c5-a45d-b00eaf57a82a",
   "metadata": {},
   "source": [
    "## Select Colab or HPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac0705a-94f2-403c-af87-5817d9bb5c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLATFORM = 'colab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714a554-a9ae-4f23-b9d0-20fc05853c08",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7080ecde-97d8-46d3-b65e-ec8b19bc79dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bz2\n",
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01365030-cfed-45cb-826f-aa57f2bd0b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpip install transformers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Mount Google Drive and CD using a shell command\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      9\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "if PLATFORM == 'colab':\n",
    "    \n",
    "    # Install Hugging Face library using a shell command\n",
    "    import os\n",
    "    os.system(\"pip install transformers\")\n",
    "\n",
    "    # Mount Google Drive and CD using a shell command\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee1580f-474a-4365-99cf-1a17727071e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForCausalLM, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e4f3c-cd18-4ffe-b6b0-5ef9c9a0329b",
   "metadata": {},
   "source": [
    "## Top level functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f2563f6-c7ef-4b04-96f6-e3734fdaed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(fname, data):\n",
    "    \"\"\"Pickle a file and compress it.\n",
    "    Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n",
    "    \"\"\"\n",
    "    with bz2.open(fname, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "def load_object(fname):\n",
    "    \"\"\"Load compressed pickle file\n",
    "    Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n",
    "    \"\"\"\n",
    "    with bz2.open(fname, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "\"\"\"Delete K valued key using dictionary comprehension and recursion. \n",
    "Source of code: https://www.geeksforgeeks.org/python-remove-k-valued-key-from-nested-dictionary\n",
    "\"\"\"\n",
    "delete_key = lambda input: {key: delete_key(value) if isinstance(value, dict) else value\n",
    "      for key, value in input.items() if key != rem_key}\n",
    "\n",
    "\n",
    "def set_seed_value(seed_value):\n",
    "    \"\"\"Create a function for setting/resetting the fixed seed value for pseudo-random generators.\n",
    "    Source: https://odsc.medium.com/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\n",
    "    Source: https://discuss.pytorch.org/t/does-pytorch-change-its-internal-seed-during-training/46505/4\n",
    "    Source: https://huggingface.co/docs/transformers/internal/trainer_utils\n",
    "    Source: https://huggingface.co/Narsil/gpt2\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed_value) # 1. Torch\n",
    "    random.seed(seed_value) # 2. Python\n",
    "    np.random.seed(seed_value) # 3. Numpy\n",
    "    # 4. HuggingFace helper function to set the seed in random , numpy , torch\n",
    "    set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29a42cd-57fc-4396-903a-83984744aec0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9151368-0369-4777-9ca0-fd2e02271efd",
   "metadata": {},
   "source": [
    "### Command line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa3f3f29-89e7-40a1-9a6c-557a20d2ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parser for command line arguments\n",
    "# Source: https://docs.python.org/3/library/argparse.html\n",
    "parser = argparse.ArgumentParser(description=\"Parse command line arguments\")\n",
    "parser.add_argument('-b',\n",
    "                    '--batch_size',\n",
    "                    metavar='batch size',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=22,\n",
    "                    help='Batch size.')\n",
    "\n",
    "parser.add_argument('-dp',\n",
    "                    '--data_parallel',\n",
    "                    metavar='data_parallel',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='N',\n",
    "                    help='Distributed inference Y/N.')\n",
    "\n",
    "parser.add_argument('-d',\n",
    "                    '--device',\n",
    "                    metavar='device',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='cpu',\n",
    "                    help='Cuda or cpu.')\n",
    "\n",
    "parser.add_argument('-f',\n",
    "                    '--filename',\n",
    "                    metavar='prompt filename',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='ID_3shot_test_set',\n",
    "                    help='Input filename.')\n",
    "\n",
    "parser.add_argument('-m',\n",
    "                    '--model',\n",
    "                    metavar='device',\n",
    "                    type=str,\n",
    "                    nargs='?',\n",
    "                    default='gpt-neo-125M',\n",
    "                    help='Pre-trained transformer model.')\n",
    "\n",
    "parser.add_argument('-n',\n",
    "                    '--number_outputs',\n",
    "                    metavar='number_outputs',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=1,\n",
    "                    help='The number of independantly sampled outputs per problem.')\n",
    "\n",
    "parser.add_argument('-t',\n",
    "                    '--temperature',\n",
    "                    metavar='temperature',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=1,\n",
    "                    help='Softmax temperature.')\n",
    "\n",
    "parser.add_argument('-k',\n",
    "                    '--topk',\n",
    "                    metavar='topk',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=0,\n",
    "                    help='The K most likely next words.')\n",
    "\n",
    "parser.add_argument('-pr',\n",
    "                    '--topp',\n",
    "                    metavar='topp',\n",
    "                    type=int,\n",
    "                    nargs='?',\n",
    "                    default=10,\n",
    "                    help='Probability threshold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8023328-5a7f-49df-9067-43410fcfca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLATFORM=='colab':\n",
    "    # args = parser.parse_args([])\n",
    "    args = parser.parse_args(['--batch_size', '1',\n",
    "                              '--data_parallel', 'N',\n",
    "                              '--device', 'cpu',\n",
    "                              '--filename', '3shot_10fold_OOD_test',\n",
    "                              '--model', 'gpt-neo-125M',\n",
    "                              '--number_outputs', '10',\n",
    "                              '--temperature', '1',\n",
    "                              '--topk', '0',\n",
    "                              '--topp', '5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc4bf450-cacb-4693-a176-cf015895df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses arguments through the parse_args() method\n",
    "if PLATFORM=='HPC':\n",
    "    args = parser.parse_args()\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "DATA_PARALLEL = args.data_parallel\n",
    "DEVICE_TYPE = args.device\n",
    "FILENAME = args.filename\n",
    "MODEL_TYPE = args.model\n",
    "NUM_OUTPUTS = args.number_outputs\n",
    "TEMPERATURE = args.temperature\n",
    "TOPK = args.topk\n",
    "TOPP = args.topp/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f51898-9382-4238-af2e-082cf23e8b47",
   "metadata": {},
   "source": [
    "### File parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c750cb28-2228-4fd3-9b01-e23190e6ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STRING = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "PROMPT_FILENAME = FILENAME + '.pickle'\n",
    "RAW_RESULTS_FILENAME = FILENAME + \"-\" + MODEL_TYPE + \"-\" + TIME_STRING + '_raw_results'\n",
    "RESULTS_FILENAME = FILENAME + \"-\" + MODEL_TYPE + \"-\" + TIME_STRING + '_results'\n",
    "\n",
    "if PLATFORM == 'colab':\n",
    "    MODEL_DIRECTORY = \"/\" + MODEL_TYPE\n",
    "    LOCAL_MODEL_DIRECTORY = \"/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K/\" + MODEL_TYPE\n",
    "    PROMPT_DIRECTORY = \"/content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\"\n",
    "\n",
    "elif platform == 'hpc':\n",
    "    ARCHIVE_FOLDER = \"/mnt/data/users/adbz866/\"\n",
    "    LOCAL_MODEL_DIRECTORY = archive_folder + model_type\n",
    "    PROMPT_DIRECTORY = \"/mnt/scratch/users/adbz866/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3812962-259f-4a9b-90d4-7cb26a4fc101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20230827-103709\n",
      "Prompt directory: /content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K\n",
      "Prompt file name: 3shot_10fold_OOD_test.pickle\n",
      "\n",
      "\n",
      "Local model directory: /content/drive/MyDrive/Colab Notebooks/Final project/2. Models/1. Transformer-based LM/1. Autoregressive LMs/2. Top-K/gpt-neo-125M\n",
      "Platform: colab\n",
      "Model: gpt-neo-125M\n",
      "Device: cpu\n",
      "Top-K: 0\n",
      "Top-P: 0.5\n",
      "Temperature: 1\n",
      "Batch size: 1\n",
      "Data parallelisation: N\n",
      "Number of outputs per problem 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Time:\", TIME_STRING)\n",
    "print(\"Prompt directory:\", PROMPT_DIRECTORY)\n",
    "print(\"Prompt file name:\", PROMPT_FILENAME)\n",
    "print(\"\\n\")\n",
    "print(\"Local model directory:\", LOCAL_MODEL_DIRECTORY)\n",
    "print(\"Platform:\", PLATFORM)\n",
    "print(\"Model:\", MODEL_TYPE)\n",
    "print(\"Device:\", DEVICE_TYPE)\n",
    "print(\"Top-K:\", TOPK)\n",
    "print(\"Top-P:\", TOPP)\n",
    "print(\"Temperature:\", TEMPERATURE)\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "print(\"Data parallelisation:\" , DATA_PARALLEL)\n",
    "print(\"Number of outputs per problem\", NUM_OUTPUTS )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260286ca-da8c-46ca-a7bf-629851e979c7",
   "metadata": {},
   "source": [
    "## Torch device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dc8bbb9-42ea-43c0-a0ef-260d821a4539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device: cpu\n",
      "Number of GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "# Source: https://pytorch.org/docs/stable/tensor_attributes.html#torch.device\n",
    "if DEVICE_TYPE == \"cuda\":\n",
    "    TORCH_DEVICE = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    TORCH_DEVICE = torch.device(DEVICE_TYPE)\n",
    "\n",
    "print(f\"Torch device: {TORCH_DEVICE}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f682508-0cd9-447c-8cac-5d06ca94602c",
   "metadata": {},
   "source": [
    "# Instantiate models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8690e-3d76-4b68-aac8-3002e3456ab6",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db222edf-e4fd-49be-b1f5-d622b3b8790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIRECTORY)\n",
    "# local_tokenizer.save_pretrained(save_directory=LOCAL_MODEL_DIRECTORY)\n",
    "# del local_tokenizer\n",
    "\n",
    "chosen_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIRECTORY)\n",
    "print(type(chosen_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d56dc87-8121-4aa3-8d5c-3cb0d6e7385d",
   "metadata": {},
   "source": [
    "## Pre-trained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c09541-21f9-4d7b-a153-2f15bf5ca3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_model = AutoModelForCausalLM.from_pretrained(MODEL_DIRECTORY)\n",
    "# local_model.save_pretrained(save_directory=LOCAL_MODEL_DIRECTORY)\n",
    "# del local_model\n",
    "\n",
    "chosen_model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_DIRECTORY)\n",
    "chosen_model.to(torch_device)\n",
    "if DEVICE_TYPE == \"cuda\" and DATA_PARALLEL == \"Y\":\n",
    "    chosen_model = torch.nn.DataParallel(chosen_model, device_ids=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa016c47-fba5-4663-ac61-982b06ca8d58",
   "metadata": {},
   "source": [
    "## Configure padding\n",
    "* EOS = end of sequence token\n",
    "* BOS = beginning of sequence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a308e-a9a3-4a14-9638-97c7db5a066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "chosen_tokenizer.padding_side = \"left\"\n",
    "chosen_tokenizer.pad_token = chosen_tokenizer.eos_token\n",
    "print(chosen_tokenizer.eos_token)\n",
    "print(chosen_tokenizer.encode(chosen_tokenizer.eos_token))\n",
    "print(chosen_tokenizer.bos_token)\n",
    "print(chosen_tokenizer.encode(chosen_tokenizer.bos_token))\n",
    "\n",
    "# Model\n",
    "if DEVICE_TYPE == \"cuda\" and  DATA_PARALLEL == \"Y\":\n",
    "    chosen_model.module.config.pad_token_id = chosen_model.module.config.eos_token_id\n",
    "else:\n",
    "    chosen_model.config.pad_token_id = chosen_model.config.eos_token_id\n",
    "print(chosen_model.config.eos_token_id)\n",
    "print(chosen_model.config.bos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510f63d-bfd1-44da-9975-9b609b52f162",
   "metadata": {},
   "source": [
    "# Import test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959249f6-c40c-4328-878e-619b94bad244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data via deseriealisation using pickle\n",
    "with open(prompt_directory + \"/\" + prompt_filename, 'rb') as f:\n",
    "    test_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18930f7-58f2-4b5d-b462-1a0d6a38437e",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d727d-b1be-4749-9eb9-8830e58b4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize prompts (with padding) using call()\n",
    "\n",
    "# Extract inputs\n",
    "# Output: [validation set 1, validation set 2, validation set 3...] where each validation set contains a list with model inputs\n",
    "tokenized_test_set = [test_sample[-1] for test_sample in test_set]\n",
    "\n",
    "# Tokenize inputs\n",
    "# Output: [validation set 1, validation set 2, validation set 3...] where each validation set contains a tensor with tokenized inputs\n",
    "tokenized_test_set = chosen_tokenizer(\n",
    "    tokenized_test_set, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "print(tokenized_test_set['input_ids'].shape)\n",
    "print(tokenized_test_set['attention_mask'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nl2vis",
   "language": "python",
   "name": "nl2vis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
