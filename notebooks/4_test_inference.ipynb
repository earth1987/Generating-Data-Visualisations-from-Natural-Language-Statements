{"cells":[{"cell_type":"markdown","id":"d314eb23-032a-4eeb-b6d2-4795b2b0cbed","metadata":{"id":"d314eb23-032a-4eeb-b6d2-4795b2b0cbed"},"source":["# Set-up"]},{"cell_type":"markdown","id":"3a733034-e959-46c5-a45d-b00eaf57a82a","metadata":{"id":"3a733034-e959-46c5-a45d-b00eaf57a82a"},"source":["## Select Colab or HPC"]},{"cell_type":"code","execution_count":17,"id":"4ac0705a-94f2-403c-af87-5817d9bb5c8d","metadata":{"id":"4ac0705a-94f2-403c-af87-5817d9bb5c8d","executionInfo":{"status":"ok","timestamp":1693494774036,"user_tz":-60,"elapsed":548,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["PLATFORM = 'colab' # colab or HPC or laptop"]},{"cell_type":"markdown","id":"e714a554-a9ae-4f23-b9d0-20fc05853c08","metadata":{"id":"e714a554-a9ae-4f23-b9d0-20fc05853c08"},"source":["## Import dependencies"]},{"cell_type":"code","execution_count":2,"id":"7080ecde-97d8-46d3-b65e-ec8b19bc79dc","metadata":{"id":"7080ecde-97d8-46d3-b65e-ec8b19bc79dc","executionInfo":{"status":"ok","timestamp":1693493570691,"user_tz":-60,"elapsed":5665,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["import argparse\n","import bz2\n","import gc\n","import json\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import random\n","from sklearn.model_selection import ParameterGrid\n","import time\n","import torch"]},{"cell_type":"code","execution_count":3,"id":"01365030-cfed-45cb-826f-aa57f2bd0b76","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01365030-cfed-45cb-826f-aa57f2bd0b76","executionInfo":{"status":"ok","timestamp":1693493601583,"user_tz":-60,"elapsed":30894,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}},"outputId":"4a330323-7aa2-4a4c-f6de-a60fb90a352e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["if PLATFORM == 'colab':\n","\n","    # Install Hugging Face library using a shell command\n","    import os\n","    os.system(\"pip install transformers\")\n","\n","    # Mount Google Drive and CD using a shell command\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    os.chdir(\"/content/drive/MyDrive/Data science jobs/2. Portfolio/3. NL2VIS/\")"]},{"cell_type":"code","execution_count":4,"id":"1ee1580f-474a-4365-99cf-1a17727071e3","metadata":{"id":"1ee1580f-474a-4365-99cf-1a17727071e3","executionInfo":{"status":"ok","timestamp":1693493605422,"user_tz":-60,"elapsed":3843,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["from transformers import AutoTokenizer, AutoConfig, AutoModel, AutoModelForCausalLM, set_seed"]},{"cell_type":"markdown","id":"232e4f3c-cd18-4ffe-b6b0-5ef9c9a0329b","metadata":{"id":"232e4f3c-cd18-4ffe-b6b0-5ef9c9a0329b"},"source":["## Top level functions"]},{"cell_type":"code","execution_count":5,"id":"9f2563f6-c7ef-4b04-96f6-e3734fdaed80","metadata":{"id":"9f2563f6-c7ef-4b04-96f6-e3734fdaed80","executionInfo":{"status":"ok","timestamp":1693493605422,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["def break_list(l, n):\n","    \"\"\" Turn a list into a list of lists with size n\n","    Source: https://www.geeksforgeeks.org/break-list-chunks-size-n-python/\n","    \"\"\"\n","    for i in range(0, len(l), n):\n","        yield l[i:i + n]\n","\n","\n","def decode_outputs(raw_sequences):\n","    \"\"\"Decode raw output sequences for a given batch\"\"\"\n","    return chosen_tokenizer.decode(raw_sequences, skip_special_tokens=False)\n","\n","\n","def save_object(fname, data):\n","    \"\"\"Pickle a file and compress it.\n","    Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n","    \"\"\"\n","    with bz2.open(fname, \"wb\") as f:\n","        pickle.dump(data, f)\n","\n","\n","def load_object(fname):\n","    \"\"\"Load compressed pickle file\n","    Source: https://betterprogramming.pub/load-fast-load-big-with-compressed-pickles-5f311584507e\n","    \"\"\"\n","    with bz2.open(fname, \"rb\") as f:\n","        data = pickle.load(f)\n","    return data\n","\n","\n","\"\"\"Delete K valued key using dictionary comprehension and recursion.\n","Source of code: https://www.geeksforgeeks.org/python-remove-k-valued-key-from-nested-dictionary\n","\"\"\"\n","delete_key = lambda input: {key: delete_key(value) if isinstance(value, dict) else value\n","      for key, value in input.items() if key != rem_key}\n","\n","\n","def set_seed_value(seed_value):\n","    \"\"\"Create a function for setting/resetting the fixed seed value for pseudo-random generators.\n","    Source: https://odsc.medium.com/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752\n","    Source: https://discuss.pytorch.org/t/does-pytorch-change-its-internal-seed-during-training/46505/4\n","    Source: https://huggingface.co/docs/transformers/internal/trainer_utils\n","    Source: https://huggingface.co/Narsil/gpt2\n","    \"\"\"\n","    torch.manual_seed(seed_value) # 1. Torch\n","    random.seed(seed_value) # 2. Python\n","    np.random.seed(seed_value) # 3. Numpy\n","    # 4. HuggingFace helper function to set the seed in random , numpy , torch\n","    set_seed(seed_value)"]},{"cell_type":"code","execution_count":6,"id":"457ac7eb-74d2-43f4-bb88-5377f7792d8f","metadata":{"id":"457ac7eb-74d2-43f4-bb88-5377f7792d8f","executionInfo":{"status":"ok","timestamp":1693493605423,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["def batch_generator(tokenized_inputs, BATCH_SIZE):\n","    \"\"\"Generator object used to deliver batches of the tokenized inputs\n","    and their associated attention masks to the model thereby reducing memory consumption.\n","    \"\"\"\n","    for i in range(0, len(tokenized_inputs), BATCH_SIZE):\n","        yield tokenized_inputs[i: i + BATCH_SIZE]\n","\n","\n","def model_inference(tokenized_inputs, BATCH_SIZE, TEMPERATURE, TOPK, TOPP, NUM_OUTPUTS, MAX_OUTPUT_LENGTH):\n","    \"\"\"This function enables batched inference using the chosen model on either the CPU or GPU.\n","    The latter includes optional parallelisation.\"\"\"\n","    set_seed_value(0)\n","    inference_results = {'Inference time': None,\n","                         'Output sequences': None\n","                        }\n","    model_outputs = []\n","\n","    # Iterate over generator objects\n","    input_ids_generator = batch_generator(tokenized_test_set['input_ids'],\n","                                           BATCH_SIZE\n","                                          )\n","    attn_mask_generator = batch_generator(tokenized_test_set['attention_mask'],\n","                                           BATCH_SIZE\n","                                          )\n","    start = time.time()\n","    generator_count = 0\n","    for input_batch, attn_batch in zip(input_ids_generator, attn_mask_generator):\n","        generator_count += 1\n","        input_batch = input_batch.to(TORCH_DEVICE)\n","        attn_batch = attn_batch.to(TORCH_DEVICE)\n","\n","        if DEVICE_TYPE == \"cuda\" and DATA_PARALLEL == \"Y\":\n","            output = chosen_model.module.generate(input_batch,\n","                                                  attention_mask=attn_batch,\n","                                                  num_beams=1,\n","                                                  do_sample=True,\n","                                                  top_k=TOPK,\n","                                                  temperature=TEMPERATURE,\n","                                                  top_p=TOPP,\n","                                                  early_stopping=True,\n","                                                  max_new_tokens=MAX_OUTPUT_LENGTH,\n","                                                  eos_token_id=50256,\n","                                                  output_scores=False,\n","                                                  return_dict_in_generate=True,\n","                                                  num_return_sequences=NUM_OUTPUTS,\n","                                                 )\n","\n","            # Release GPU memory\n","            # Step 1: Detach tensors, create a copy on the CPU and overwrite variables\n","            # Step 2: Deleting unused objects by trigerring a manual garbage collection process and releasing all unoccupied cached memory\n","            output.sequences = output.sequences.detach().cpu()\n","            attn_batch = attn_batch.detach().cpu()\n","            input_batch = input_batch.detach().cpu()\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        elif DEVICE_TYPE == \"cuda\" and DATA_PARALLEL == \"N\":\n","            output = chosen_model.generate(input_batch,\n","                                           attention_mask=attn_batch,\n","                                           num_beams=1,\n","                                           do_sample=True,\n","                                           top_k=TOPK,\n","                                           temperature=TEMPERATURE,\n","                                           top_p=TOPP,\n","                                           max_new_tokens=MAX_OUTPUT_LENGTH,\n","                                           eos_token_id=50256,\n","                                           output_scores=False,\n","                                           return_dict_in_generate=True,\n","                                           num_return_sequences=NUM_OUTPUTS,\n","                                          )\n","\n","            # Release GPU memory\n","            # Step 1: Detach tensors, create a copy on the CPU and overwrite variables\n","            # Step 2: Deleting unused objects by trigerring a manual garbage collection process and releasing all unoccupied cached memory\n","            output.sequences = output.sequences.detach().cpu()\n","            attn_mask = attn_batch.detach().cpu()\n","            input_batch = input_batch.detach().cpu()\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","        else:\n","            output = chosen_model.generate(input_batch,\n","                                           attention_mask=attn_batch,\n","                                           num_beams=1,\n","                                           do_sample=True,\n","                                           top_k=TOPK,\n","                                           temperature=TEMPERATURE,\n","                                           top_p=TOPP,\n","                                           early_stopping=True,\n","                                           max_new_tokens=MAX_OUTPUT_LENGTH,\n","                                           eos_token_id=50256,\n","                                           output_scores=False,\n","                                           return_dict_in_generate=True,\n","                                           num_return_sequences=NUM_OUTPUTS,\n","                                          )\n","\n","        model_outputs.append(output.sequences)\n","        del output\n","\n","    end = time.time()\n","    inference_time = end - start\n","    inference_results['Inference time'] = inference_time\n","    model_outputs = [output_sequence for output_sequence in model_outputs]\n","    inference_results['Output sequences'] = model_outputs\n","    return inference_results\n","\n","\n","def sequence_probability(tokenized_inputs, output_sequences, output_scores):\n","    \"\"\"Calculate sequence probabilities from output scores.\n","    Code adapted from: 1) https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n","    2) https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F17187507%2Fwhy-use-softmax-as-opposed-to-standard-normalization\n","    Note:\n","    model_inputs = tokenized_prompts = {input_ids: tensor([prompt 1], [prompt 2]...), attention_mask: tensor([prompt 1], [prompt 2]...)}\n","    model_outputs = output_batch = SampleDecoderOnlyOutput([(sequences, tensor([output 1], [output 2]...), (scores, tensor([output 1], [output 2]...))]))\n","    \"\"\"\n","    prompt_length = tokenized_inputs.shape[-1]\n","    vocab_size = output_scores[0].shape[-1]\n","    generated_tokens = output_sequences[:, prompt_length:] # generated tokens ONLY (i.e. exclude prompt)\n","\n","    # Stack the logits generated at each step and calculate probabilities across the entire vocab at each step\n","    # Note: PyTorch torch.stack() method joins (concatenates) a sequence of tensors (two or more tensors) along a new dimension.\n","    vocab_logits = torch.stack(output_scores, dim=1)\n","    vocab_probs = vocab_logits.softmax(-1)\n","\n","    # Collect probability of generated tokens & calculate sequence probability\n","    generated_tokens_probs  = torch.gather(vocab_probs, 2,\n","                                          generated_tokens[:, :, None]).squeeze(-1)\n","    generated_sequence_probs = generated_tokens_probs.prod(-1)\n","\n","    # # Print summary information\n","    # print(f\"Length of padded prompts: {prompt_length}\")\n","    # print(f\"Number of generated sequences: {generated_tokens.shape[0]}\")\n","    # print(f\"Length of generated sequences: {generated_tokens.shape[-1]}\")\n","    # print(f\"Tensor shape - Generated sequences: {generated_tokens.shape}\")\n","    # print(\"\\n\")\n","    # print(f\"Vocab size: {vocab_size}\")\n","    # print(f\"Tensor shape - Vocab token logits at each step: {vocab_logits.shape}\")\n","    # print(f\"Tensor shape - Vocab token probabilities at each step: {vocab_logits.shape}\")\n","    # print(f\"Tensor shape - Generated token probabilities: {generated_tokens_probs.shape}\")\n","    # print(f\"Tensor shape - Generated sequence probabilities: {generated_sequence_probs.shape}\")\n","    # print(f\"Sequence probabilities: {generated_sequence_probs.tolist()}\")\n","\n","    return generated_sequence_probs.tolist()"]},{"cell_type":"markdown","id":"f29a42cd-57fc-4396-903a-83984744aec0","metadata":{"id":"f29a42cd-57fc-4396-903a-83984744aec0"},"source":["## Parameters"]},{"cell_type":"markdown","id":"b9151368-0369-4777-9ca0-fd2e02271efd","metadata":{"id":"b9151368-0369-4777-9ca0-fd2e02271efd"},"source":["### Command line arguments"]},{"cell_type":"code","execution_count":7,"id":"fa3f3f29-89e7-40a1-9a6c-557a20d2ab0b","metadata":{"id":"fa3f3f29-89e7-40a1-9a6c-557a20d2ab0b","executionInfo":{"status":"ok","timestamp":1693493605423,"user_tz":-60,"elapsed":10,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Create parser for command line arguments\n","# Source: https://docs.python.org/3/library/argparse.html\n","parser = argparse.ArgumentParser(description=\"Parse command line arguments\")\n","parser.add_argument('-b',\n","                    '--batch_size',\n","                    metavar='batch size',\n","                    type=int,\n","                    nargs='?',\n","                    default=22,\n","                    help='Batch size.')\n","\n","parser.add_argument('-dp',\n","                    '--data_parallel',\n","                    metavar='data_parallel',\n","                    type=str,\n","                    nargs='?',\n","                    default='N',\n","                    help='Distributed inference Y/N.')\n","\n","parser.add_argument('-d',\n","                    '--device',\n","                    metavar='device',\n","                    type=str,\n","                    nargs='?',\n","                    default='cpu',\n","                    help='Cuda or cpu.')\n","\n","parser.add_argument('-f',\n","                    '--filename',\n","                    metavar='prompt filename',\n","                    type=str,\n","                    nargs='?',\n","                    default='ID_3shot_test_set',\n","                    help='Input filename.')\n","\n","parser.add_argument('-m',\n","                    '--model',\n","                    metavar='device',\n","                    type=str,\n","                    nargs='?',\n","                    default='gpt-neo-125M',\n","                    help='Pre-trained transformer model.')\n","\n","parser.add_argument('-n',\n","                    '--number_outputs',\n","                    metavar='number_outputs',\n","                    type=int,\n","                    nargs='?',\n","                    default=1,\n","                    help='The number of independantly sampled outputs per problem.')\n","\n","parser.add_argument('-t',\n","                    '--temperature',\n","                    metavar='temperature',\n","                    type=int,\n","                    nargs='?',\n","                    default=1,\n","                    help='Softmax temperature.')\n","\n","parser.add_argument('-k',\n","                    '--topk',\n","                    metavar='topk',\n","                    type=int,\n","                    nargs='?',\n","                    default=0,\n","                    help='The K most likely next words.')\n","\n","parser.add_argument('-pr',\n","                    '--topp',\n","                    metavar='topp',\n","                    type=int,\n","                    nargs='?',\n","                    default=10,\n","                    help='Probability threshold');"]},{"cell_type":"code","execution_count":8,"id":"d8023328-5a7f-49df-9067-43410fcfca31","metadata":{"id":"d8023328-5a7f-49df-9067-43410fcfca31","executionInfo":{"status":"ok","timestamp":1693493605424,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Parse arguments via the parse_args() method\n","if (PLATFORM=='colab') or (PLATFORM=='laptop'):\n","    # args = parser.parse_args([])\n","    args = parser.parse_args(['--batch_size', '1',\n","                              '--data_parallel', 'N',\n","                              '--device', 'cuda',\n","                              '--filename', 'OOD_3shot_test_set',\n","                              '--model', 'gpt-j-6b',\n","                              '--number_outputs', '10',\n","                              '--temperature', '1',\n","                              '--topk', '0',\n","                              '--topp', '3'])\n","elif PLATFORM=='HPC':\n","    args = parser.parse_args()"]},{"cell_type":"code","execution_count":9,"id":"fc4bf450-cacb-4693-a176-cf015895df04","metadata":{"id":"fc4bf450-cacb-4693-a176-cf015895df04","executionInfo":{"status":"ok","timestamp":1693493605424,"user_tz":-60,"elapsed":10,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["BATCH_SIZE = args.batch_size\n","DATA_PARALLEL = args.data_parallel\n","DEVICE_TYPE = args.device\n","FILENAME = args.filename\n","MODEL_TYPE = args.model\n","NUM_OUTPUTS = args.number_outputs\n","TEMPERATURE = args.temperature\n","TOPK = args.topk\n","TOPP = args.topp/10"]},{"cell_type":"markdown","id":"b7f51898-9382-4238-af2e-082cf23e8b47","metadata":{"id":"b7f51898-9382-4238-af2e-082cf23e8b47"},"source":["### File parameters"]},{"cell_type":"code","execution_count":10,"id":"c750cb28-2228-4fd3-9b01-e23190e6ddd5","metadata":{"id":"c750cb28-2228-4fd3-9b01-e23190e6ddd5","executionInfo":{"status":"ok","timestamp":1693493605424,"user_tz":-60,"elapsed":10,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["TIME_STRING = time.strftime(\"%Y%m%d-%H%M%S\")\n","PROMPT_FILENAME = FILENAME + '.pickle'\n","RAW_RESULTS_FILENAME = FILENAME + \"-\" + MODEL_TYPE + \"-\" + TIME_STRING + '_raw_results'\n","RESULTS_FILENAME = FILENAME + \"-\" + MODEL_TYPE + \"-\" + TIME_STRING + '_results'\n","\n","if PLATFORM == 'colab':\n","    MODEL_DIRECTORY =  'EleutherAI/' + MODEL_TYPE\n","    LOCAL_MODEL_DIRECTORY = '/content/drive/MyDrive/Data science jobs/2. Portfolio/3. NL2VIS/' + MODEL_TYPE\n","    PROMPT_DIRECTORY = '/content/drive/MyDrive/Data science jobs/2. Portfolio/3. NL2VIS/'\n","\n","elif PLATFORM == 'hpc':\n","    ARCHIVE_FOLDER = \"/mnt/data/users/adbz866/\"\n","    LOCAL_MODEL_DIRECTORY = archive_folder + MODEL_TYPE\n","    PROMPT_DIRECTORY = \"/mnt/scratch/users/adbz866/\"\n","\n","elif PLATFORM == 'laptop':\n","    MODEL_DIRECTORY =  'EleutherAI/' + MODEL_TYPE\n","    LOCAL_MODEL_DIRECTORY = 'C:/Users/billy/OneDrive/Documents/Python Scripts/1. Portfolio/1. NL2VIS/' + MODEL_TYPE\n","    PROMPT_DIRECTORY = 'C:/Users/billy/OneDrive/Documents/Python Scripts/1. Portfolio/1. NL2VIS/'"]},{"cell_type":"code","execution_count":11,"id":"a3812962-259f-4a9b-90d4-7cb26a4fc101","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3812962-259f-4a9b-90d4-7cb26a4fc101","executionInfo":{"status":"ok","timestamp":1693493605425,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}},"outputId":"c71c1635-0154-4f79-cf43-2ad918e8acd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Time: 20230831-145324\n","Prompt directory: /content/drive/MyDrive/Data science jobs/2. Portfolio/3. NL2VIS/\n","Prompt filename: OOD_3shot_test_set.pickle\n","\n","\n","Local model directory: /content/drive/MyDrive/Data science jobs/2. Portfolio/3. NL2VIS/gpt-j-6b\n","Platform: colab\n","Model: gpt-j-6b\n","Device: cuda\n","Top-K: 0\n","Top-P: 0.3\n","Temperature: 1\n","Batch size: 1\n","Data parallelisation: N\n","Number of outputs per problem 10\n","\n","\n","Raw results filename OOD_3shot_test_set-gpt-j-6b-20230831-145324_raw_results\n","Results filename: OOD_3shot_test_set-gpt-j-6b-20230831-145324_results\n"]}],"source":["print(\"Time:\", TIME_STRING)\n","print(\"Prompt directory:\", PROMPT_DIRECTORY)\n","print(\"Prompt filename:\", PROMPT_FILENAME)\n","print(\"\\n\")\n","print(\"Local model directory:\", LOCAL_MODEL_DIRECTORY)\n","print(\"Platform:\", PLATFORM)\n","print(\"Model:\", MODEL_TYPE)\n","print(\"Device:\", DEVICE_TYPE)\n","print(\"Top-K:\", TOPK)\n","print(\"Top-P:\", TOPP)\n","print(\"Temperature:\", TEMPERATURE)\n","print(\"Batch size:\", BATCH_SIZE)\n","print(\"Data parallelisation:\" , DATA_PARALLEL)\n","print(\"Number of outputs per problem\", NUM_OUTPUTS )\n","print(\"\\n\")\n","print(\"Raw results filename\", RAW_RESULTS_FILENAME)\n","print(\"Results filename:\", RESULTS_FILENAME)"]},{"cell_type":"markdown","id":"260286ca-da8c-46ca-a7bf-629851e979c7","metadata":{"id":"260286ca-da8c-46ca-a7bf-629851e979c7"},"source":["## Torch device\n","* The torch.device objects represents the device on which torch.tensors will be allocated.ce"]},{"cell_type":"code","execution_count":12,"id":"9dc8bbb9-42ea-43c0-a0ef-260d821a4539","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dc8bbb9-42ea-43c0-a0ef-260d821a4539","executionInfo":{"status":"ok","timestamp":1693493605425,"user_tz":-60,"elapsed":8,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}},"outputId":"b413552f-e8a0-4f95-ebbd-0184b72b0c87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Torch device: cuda:0\n","Number of GPUs: 1\n"]}],"source":["# Source: https://pytorch.org/docs/stable/tensor_attributes.html#torch.device\n","if DEVICE_TYPE == \"cuda\":\n","    TORCH_DEVICE = torch.device(\"cuda:0\")\n","else:\n","    TORCH_DEVICE = torch.device(DEVICE_TYPE)\n","\n","print(f\"Torch device: {TORCH_DEVICE}\")\n","print(f\"Number of GPUs: {torch.cuda.device_count()}\")"]},{"cell_type":"markdown","id":"9f682508-0cd9-447c-8cac-5d06ca94602c","metadata":{"id":"9f682508-0cd9-447c-8cac-5d06ca94602c"},"source":["# Instantiate models"]},{"cell_type":"markdown","id":"57e8690e-3d76-4b68-aac8-3002e3456ab6","metadata":{"id":"57e8690e-3d76-4b68-aac8-3002e3456ab6"},"source":["## Tokenizer\n","* Tokenize with padding."]},{"cell_type":"code","execution_count":13,"id":"db222edf-e4fd-49be-b1f5-d622b3b8790f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"db222edf-e4fd-49be-b1f5-d622b3b8790f","executionInfo":{"status":"ok","timestamp":1693493607519,"user_tz":-60,"elapsed":2101,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}},"outputId":"4ce0a204-ddb9-4448-d5ea-2fb706508fb0"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast'>\n"]}],"source":["# local_tokenizer = AutoTokenizer.from_pretrained(MODEL_DIRECTORY)\n","# local_tokenizer.save_pretrained(save_directory=LOCAL_MODEL_DIRECTORY)\n","# del local_tokenizer\n","\n","chosen_tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIRECTORY)\n","print(type(chosen_tokenizer))"]},{"cell_type":"markdown","id":"7d56dc87-8121-4aa3-8d5c-3cb0d6e7385d","metadata":{"id":"7d56dc87-8121-4aa3-8d5c-3cb0d6e7385d"},"source":["## Pre-trained transformer\n","* Data parallelism is implemented as per [here](https://stackoverflow.com/questions/61736317/huggingface-transformers-gpt2-generate-multiple-gpus). This means the input will be distributed across the available GPUs."]},{"cell_type":"code","execution_count":16,"id":"b2c09541-21f9-4d7b-a153-2f15bf5ca3cc","metadata":{"id":"b2c09541-21f9-4d7b-a153-2f15bf5ca3cc","executionInfo":{"status":"error","timestamp":1693494244058,"user_tz":-60,"elapsed":166592,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}},"colab":{"base_uri":"https://localhost:8080/","height":457,"referenced_widgets":["b677031faac7456b9c0904a95c5e6c51","baaf46eace9a493d93962a47469d568c","7fb83990cc284b9d91c0a34a34035945","5c850e4f9f5f4bb181c3a55f55fb0921","4c313cb92b7d4c4695b1e79f6eff3fb7","d7378f21bdb449358e3fe648c792daf9","9e4db03f6bf8457cb01ca84440ced087","395cdd15d34543e5ba2c084a9f9b3b25","5f40fe40190340208e82f73b094272f5","42cf5463384f4c3e80f1a9d551a6665f","d108442a6c734c9e97d72ac67223e530"]},"outputId":"c8e8a221-ac72-4052-ca95-1846217cd495"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b677031faac7456b9c0904a95c5e6c51"}},"metadata":{}},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-84a6a2b69a4f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchosen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLOCAL_MODEL_DIRECTORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mchosen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTORCH_DEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mDEVICE_TYPE\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mDATA_PARALLEL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Y\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mchosen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2012\u001b[0m             )\n\u001b[1;32m   2013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2014\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 15.77 GiB total capacity; 15.35 GiB already allocated; 122.12 MiB free; 15.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# local_model = AutoModelForCausalLM.from_pretrained(MODEL_DIRECTORY)\n","# local_model.save_pretrained(save_directory=LOCAL_MODEL_DIRECTORY)\n","# del local_model\n","\n","chosen_model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_DIRECTORY)\n","chosen_model.to(TORCH_DEVICE)\n","if DEVICE_TYPE == \"cuda\" and DATA_PARALLEL == \"Y\":\n","    chosen_model = torch.nn.DataParallel(chosen_model, device_ids=[0, 1])"]},{"cell_type":"markdown","id":"aa016c47-fba5-4663-ac61-982b06ca8d58","metadata":{"id":"aa016c47-fba5-4663-ac61-982b06ca8d58"},"source":["## Configure padding\n","* EOS = end of sequence token\n","* BOS = beginning of sequence token"]},{"cell_type":"code","execution_count":null,"id":"531a308e-a9a3-4a14-9638-97c7db5a066f","metadata":{"id":"531a308e-a9a3-4a14-9638-97c7db5a066f","executionInfo":{"status":"aborted","timestamp":1693493607866,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Tokenizer\n","chosen_tokenizer.padding_side = \"left\"\n","chosen_tokenizer.pad_token = chosen_tokenizer.eos_token\n","print(chosen_tokenizer.eos_token)\n","print(chosen_tokenizer.encode(chosen_tokenizer.eos_token))\n","print(chosen_tokenizer.bos_token)\n","print(chosen_tokenizer.encode(chosen_tokenizer.bos_token))\n","\n","# Model\n","if DEVICE_TYPE == \"cuda\" and  DATA_PARALLEL == \"Y\":\n","    chosen_model.module.config.pad_token_id = chosen_model.module.config.eos_token_id\n","else:\n","    chosen_model.config.pad_token_id = chosen_model.config.eos_token_id\n","print(chosen_model.config.eos_token_id)\n","print(chosen_model.config.bos_token_id)"]},{"cell_type":"markdown","id":"4510f63d-bfd1-44da-9975-9b609b52f162","metadata":{"id":"4510f63d-bfd1-44da-9975-9b609b52f162"},"source":["# Import test data"]},{"cell_type":"code","execution_count":null,"id":"959249f6-c40c-4328-878e-619b94bad244","metadata":{"id":"959249f6-c40c-4328-878e-619b94bad244","executionInfo":{"status":"aborted","timestamp":1693493607866,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Load test data via deseriealisation using pickle\n","with open(PROMPT_DIRECTORY + \"/\" + PROMPT_FILENAME, 'rb') as f:\n","    test_set = pickle.load(f)"]},{"cell_type":"markdown","id":"a18930f7-58f2-4b5d-b462-1a0d6a38437e","metadata":{"id":"a18930f7-58f2-4b5d-b462-1a0d6a38437e"},"source":["# Pre-processing"]},{"cell_type":"code","execution_count":null,"id":"40e60ca3-4601-4fe5-87b1-0f783577cf83","metadata":{"id":"40e60ca3-4601-4fe5-87b1-0f783577cf83","executionInfo":{"status":"aborted","timestamp":1693493607867,"user_tz":-60,"elapsed":12,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Tokenize prompts (with padding) using call()\n","tokenized_test_set = test_set['model_input'].tolist()\n","tokenized_test_set = chosen_tokenizer(tokenized_test_set,\n","                                      return_tensors=\"pt\",\n","                                      padding=True)\n","\n","print(tokenized_test_set['input_ids'].shape)\n","print(tokenized_test_set['attention_mask'].shape)"]},{"cell_type":"markdown","id":"bdd9b2e9-830b-47cd-a61f-515ecf161a1a","metadata":{"id":"bdd9b2e9-830b-47cd-a61f-515ecf161a1a"},"source":["# Inference"]},{"cell_type":"markdown","id":"c7153c70-c3be-4e6e-9ae6-a30d00e5900c","metadata":{"id":"c7153c70-c3be-4e6e-9ae6-a30d00e5900c"},"source":["## Stopping criteria\n","* The early stopping criteria is based on *max_new_tokens* and the *EOS* token.\n","\n","* Max_new_tokens is estimated by determining the number of tokens in the largest specification example. 10% more tokens is then added."]},{"cell_type":"code","execution_count":null,"id":"94a164cb-e138-4008-aeb8-1e8890a5a11d","metadata":{"id":"94a164cb-e138-4008-aeb8-1e8890a5a11d","executionInfo":{"status":"aborted","timestamp":1693493607867,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Calculate maximum specification length\n","i = 0\n","MAX_OUTPUT_LENGTH = 0\n","for spec in test_set['spec'].tolist():\n","    i += 1\n","    length = chosen_tokenizer(json.dumps(spec),\n","                              return_tensors=\"pt\",\n","                              padding=False).input_ids.shape[1]\n","\n","    if length > MAX_OUTPUT_LENGTH:\n","        MAX_OUTPUT_LENGTH = length\n","\n","MAX_OUTPUT_LENGTH += MAX_OUTPUT_LENGTH*0.1\n","MAX_OUTPUT_LENGTH = round(MAX_OUTPUT_LENGTH, 0)\n","print(f\"Max output length: {MAX_OUTPUT_LENGTH}\")"]},{"cell_type":"markdown","id":"72307257-e970-4a78-b227-88cbc7be2430","metadata":{"id":"72307257-e970-4a78-b227-88cbc7be2430"},"source":["## Inference\n","\n","Sequence probabilities are not output below. However, they can be using the *sequence_probabilities()* function defined earlier. In this instance, note the below:\n","* Sequence probabilities are calculated from output scores.\n","* However, for each generated token, output scores are provided across all tokens in the model's vocab (~50,000). Storing output scores for every sample therefore consumes a lot of memory.\n","* To counter this, calculate sequence probabilities immediately for each batch before deleting output scores using the function provided."]},{"cell_type":"code","execution_count":null,"id":"78cc358d-a610-4a10-961e-23ca2088c43c","metadata":{"id":"78cc358d-a610-4a10-961e-23ca2088c43c","executionInfo":{"status":"aborted","timestamp":1693493607868,"user_tz":-60,"elapsed":12,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["inference_results = model_inference(tokenized_test_set, BATCH_SIZE, TEMPERATURE, TOPK, TOPP, NUM_OUTPUTS, MAX_OUTPUT_LENGTH)\n","# print(inference_results.keys())\n","# print(len(inference_results['Output sequences']))"]},{"cell_type":"markdown","id":"d58bf279-48d2-4cb0-a2d0-2a915e17de00","metadata":{"id":"d58bf279-48d2-4cb0-a2d0-2a915e17de00"},"source":["# Process outputs"]},{"cell_type":"code","execution_count":null,"id":"1a2f7eb4-f324-426a-9333-2f75c56781ad","metadata":{"id":"1a2f7eb4-f324-426a-9333-2f75c56781ad","executionInfo":{"status":"aborted","timestamp":1693493607868,"user_tz":-60,"elapsed":12,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Decode & flatten batches\n","inference_results['Decoded sequences'] = []\n","for batch in inference_results['Output sequences']:\n","    inference_results['Decoded sequences'].append([decode_outputs(output) for output in batch])\n","inference_results['Decoded sequences'] = [decode for batch in inference_results['Decoded sequences'] for decode in batch]\n","\n","# Group together independantly sampled outputs\n","if NUM_OUTPUTS > 1:\n","    inference_results['Decoded sequences'] = list(break_list(inference_results['Decoded sequences'],\n","                                                             NUM_OUTPUTS\n","                                                            )\n","                                                 )"]},{"cell_type":"markdown","id":"bbaa2454-bc80-4209-9a51-5e3eddee4e2c","metadata":{"id":"bbaa2454-bc80-4209-9a51-5e3eddee4e2c"},"source":["# Save outputs\n","* To save memory, unnecessary keys are removed from the output containing decoded sequences."]},{"cell_type":"code","execution_count":null,"id":"f7612acb-d164-4c6d-b71d-3256aa4a1ef4","metadata":{"id":"f7612acb-d164-4c6d-b71d-3256aa4a1ef4","executionInfo":{"status":"aborted","timestamp":1693493607869,"user_tz":-60,"elapsed":11,"user":{"displayName":"Billy Pitchford","userId":"05624495549174061003"}}},"outputs":[],"source":["# Raw data\n","save_object(RAW_RESULTS_FILENAME,\n","            inference_results)\n","\n","# Decoded output\n","for rem_key in ['Output scores', 'Input sequences', 'Output sequences']:\n","    inference_results = delete_key(inference_results)\n","save_object(RESULTS_FILENAME,\n","            inference_results)"]}],"metadata":{"kernelspec":{"display_name":"nl2vis","language":"python","name":"nl2vis"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"A100","machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b677031faac7456b9c0904a95c5e6c51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_baaf46eace9a493d93962a47469d568c","IPY_MODEL_7fb83990cc284b9d91c0a34a34035945","IPY_MODEL_5c850e4f9f5f4bb181c3a55f55fb0921"],"layout":"IPY_MODEL_4c313cb92b7d4c4695b1e79f6eff3fb7"}},"baaf46eace9a493d93962a47469d568c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7378f21bdb449358e3fe648c792daf9","placeholder":"â€‹","style":"IPY_MODEL_9e4db03f6bf8457cb01ca84440ced087","value":"Loading checkpoint shards: 100%"}},"7fb83990cc284b9d91c0a34a34035945":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_395cdd15d34543e5ba2c084a9f9b3b25","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f40fe40190340208e82f73b094272f5","value":3}},"5c850e4f9f5f4bb181c3a55f55fb0921":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42cf5463384f4c3e80f1a9d551a6665f","placeholder":"â€‹","style":"IPY_MODEL_d108442a6c734c9e97d72ac67223e530","value":" 3/3 [01:42&lt;00:00, 31.82s/it]"}},"4c313cb92b7d4c4695b1e79f6eff3fb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7378f21bdb449358e3fe648c792daf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e4db03f6bf8457cb01ca84440ced087":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"395cdd15d34543e5ba2c084a9f9b3b25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f40fe40190340208e82f73b094272f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"42cf5463384f4c3e80f1a9d551a6665f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d108442a6c734c9e97d72ac67223e530":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}